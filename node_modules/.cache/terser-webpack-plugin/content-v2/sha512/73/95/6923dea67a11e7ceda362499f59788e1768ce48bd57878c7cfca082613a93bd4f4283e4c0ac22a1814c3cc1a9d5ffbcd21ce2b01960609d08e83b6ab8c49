{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{283:function(t,s,a){t.exports=a.p+\"assets/img/namenode.f626aa82.png\"},284:function(t,s,a){t.exports=a.p+\"assets/img/HDFS.fa7eae62.png\"},285:function(t,s,a){t.exports=a.p+\"assets/img/checkpoint.924aeadc.png\"},286:function(t,s,a){t.exports=a.p+\"assets/img/write.75a0eb48.png\"},287:function(t,s,a){t.exports=a.p+\"assets/img/read.fbff4e11.png\"},288:function(t,s,a){t.exports=a.p+\"assets/img/1240.116f080e.png\"},289:function(t,s,a){t.exports=a.p+\"assets/img/mapreduce.5726c0ba.png\"},290:function(t,s,a){t.exports=a.p+\"assets/img/yarn.a055ed6e.png\"},291:function(t,s,a){t.exports=a.p+\"assets/img/wc_res.94dccd2d.png\"},342:function(t,s,a){\"use strict\";a.r(s);var n=a(28),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":t.$parent.slotKey}},[n(\"h1\",{attrs:{id:\"hadoop\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop\"}},[t._v(\"#\")]),t._v(\" Hadoop\")]),t._v(\" \"),n(\"h2\",{attrs:{id:\"大数据概念\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#大数据概念\"}},[t._v(\"#\")]),t._v(\" 大数据概念\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"什么是大数据？\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#什么是大数据？\"}},[t._v(\"#\")]),t._v(\" 什么是大数据？\")]),t._v(\" \"),n(\"blockquote\",[n(\"p\",[t._v(\"KB - MB - GB - TB - PB - EB - ZB - YB - DB - NB\")])]),t._v(\" \"),n(\"p\",[t._v(\"一般来说达到 TB ，或日增达到 GB 就属于大数据了\\nMySQL 单表超过 500万 条数据\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"大数据特点？\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#大数据特点？\"}},[t._v(\"#\")]),t._v(\" 大数据特点？\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"大数据的 5V 特性：\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"Volume：巨大的数据量\")]),t._v(\"\\n集中储存/集中计算已经无法处理巨大的数据量。\\n数据量呈指数增长：地震、录井 、石油钻塔的传感器一个月产生的数据量比全球所有的电影加在一起还要多。\\n新浪微博用户数2.5亿+，高峰每天几亿条。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"Variety：非结构化数据多样性\")]),t._v(\"\\n文本/图片/视频/文档等，如诸如微地震，电磁以及光纤分布式温度监测（DTS） 。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"Velocity：数据增长速度快\")]),t._v(\"\\n用户基数庞大/设备数量众多/实时海量/数据指数级别增长。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"Valueless：数据价值密度低\")]),t._v(\"\\n每个钻井平台有 40,000 传感器,但是通常只有 10% 的数据使用到。\\n每个深水钻井平台的投资可达到$150M,能有效利用所有的数据非常关键,关系到安全与优化运营 。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"Veracity：数据质量\")]),t._v(\"\\n数据的准确性和可信赖度，即数据的质量。\")])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"面临了哪些问题-如何解决？\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#面临了哪些问题-如何解决？\"}},[t._v(\"#\")]),t._v(\" 面临了哪些问题,如何解决？\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"数据如何存储 - HDFS\")]),t._v(\" \"),n(\"li\",[t._v(\"数据如何计算 - MapReduce\")]),t._v(\" \"),n(\"li\",[t._v(\"资源如何管理（CPU 内存 网络资源）- YARN\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"大数据应用场景\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#大数据应用场景\"}},[t._v(\"#\")]),t._v(\" 大数据应用场景\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"个人推荐\")]),t._v(\"\\n根据用户喜好，推荐相关兴趣内容\\n千人一面：范围广、精度粗\\n一人一面：范围小、精度高\\n一人千面：兴趣内容范围大、精度高\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"风控\")]),t._v(\"\\n金融系统、银行、互联网金融 - 实时流处理\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"成本预测\")]),t._v(\"\\n根据近期销售和市场数据，预测成本，做出规划\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"气候预测\")]),t._v(\"\\n根据以往气象信息，预测近期气象变化，和推测之后气候异常\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"人工智能\")]),t._v(\"\\n无人汽车：百度、特斯拉、Google\\n智能助手：小爱、小度\\n物流机器人\")])])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"hadoop-的诞生\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop-的诞生\"}},[t._v(\"#\")]),t._v(\" Hadoop 的诞生\")]),t._v(\" \"),n(\"p\",[t._v(\"由 Apache 组织提供的一个开源的大数据解决方案。\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"hadoop-的起源\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop-的起源\"}},[t._v(\"#\")]),t._v(\" Hadoop 的起源\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"2003-2004年，Google公布了部分GFS和MapReduce思想的细节，受此启发的Doug Cutting等人用2年的业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。然后Yahoo招安Doug Gutting及其项目。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"2005年，Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"2006年2月被分离出来，成为一套完整独立的软件，起名为Hadoop。Hadoop名字不是一个缩写，而是一个生造出来的词。是Hadoop之父Doug Cutting儿子毛绒玩具象命名的。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"Hadoop的成长过程\\nLucene–>Nutch—>Hadoop\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"总结起来，Hadoop起源于Google的三大论文\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"GFS：Google的分布式文件系统Google File System\")]),t._v(\" \"),n(\"li\",[t._v(\"MapReduce：Google的MapReduce开源分布式并行计算框架\")]),t._v(\" \"),n(\"li\",[t._v(\"BigTable：一个大型的分布式数据库\")])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"演变关系\\nGFS—->HDFS\\nGoogle MapReduce—->Hadoop MapReduce\\nBigTable—->HBase\")])])]),t._v(\" \"),n(\"p\",[t._v(\"狭义上来说，hadoop就是单独指代hadoop这个软件，\\n广义上来说，hadoop指代大数据的一个生态圈，包括很多其他的软件\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"hadoop-的历史版本\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop-的历史版本\"}},[t._v(\"#\")]),t._v(\" Hadoop 的历史版本\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"strong\",[t._v(\"0.x系列版本\")]),t._v(\"：hadoop当中最早的一个开源版本，在此基础上演变而来的1.x以及2.x的版本\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"1.x版本系列\")]),t._v(\"：hadoop版本当中的第二代开源版本，主要修复0.x版本的一些bug等\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"2.x版本系列\")]),t._v(\"：架构产生重大变化，引入了yarn平台等许多新特性\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"hadoop-生态圈\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hadoop-生态圈\"}},[t._v(\"#\")]),t._v(\" Hadoop 生态圈\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"strong\",[t._v(\"HDFS\")]),t._v(\"：Hadoop Distribute FileSystem\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"MapReduce\")]),t._v(\"：Hadoop中的分布式计算框架，实现对海量数据的并行分析和计算。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Hbase\")]),t._v(\"：基于HDFS的列式存储的 NoSQL 数据库。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Hive\")]),t._v(\"：简化大数据开发，可以将 SQL 语法翻译成 MR 任务。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Flume\")]),t._v(\"：分布式的日志收集系统，用于收集海量数据，将其存储到 FS 中。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Kafka\")]),t._v(\"：分布式的消息系统，实现分布式解耦和海量数据缓冲。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Zookeeper\")]),t._v(\"：分布式协调服务，用于服务注册中心、配置中心、集群选举、状态监测、分布式锁等。\")])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"大数据解决方案\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#大数据解决方案\"}},[t._v(\"#\")]),t._v(\" 大数据解决方案\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"strong\",[t._v(\"MR\")]),t._v(\"：代表基于\"),n(\"strong\",[t._v(\"磁盘\")]),t._v(\"的大数据离线批处理的解决方案 - 延迟较高\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Spark\")]),t._v(\"：代表基于\"),n(\"strong\",[t._v(\"内存\")]),t._v(\"的大数据静态批处理的解决方案 - 几乎是MR的10倍以上\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"Storm/Spark Streaming/Flink/Kafka Streaming\")]),t._v(\"：实时流处理框架，达到对记录级别的数据显示和毫秒级处理\")])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"hdfs-分布式系统配置\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-分布式系统配置\"}},[t._v(\"#\")]),t._v(\" HDFS 分布式系统配置\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"核心配置参数：\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"指定 hadoop 的默认文件系统为：hdfs\")]),t._v(\" \"),n(\"li\",[t._v(\"指定 hdfs 的 namenode 节点是哪台机器\")]),t._v(\" \"),n(\"li\",[t._v(\"指定 namenode 软件存储元数据的本地目录\")]),t._v(\" \"),n(\"li\",[t._v(\"指定 datanode 软件存储文件块的本地目录\")])]),t._v(\" \"),n(\"ol\",[n(\"li\",[n(\"p\",[t._v(\"环境配置文件hadoop-env.sh\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-shell extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"# The java implementation to use.\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token builtin class-name\"}},[t._v(\"export\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token assign-left variable\"}},[t._v(\"JAVA_HOME\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"/opt/soft/jdk1.8.0_211\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token builtin class-name\"}},[t._v(\"export\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token assign-left variable\"}},[t._v(\"HADOOP_CONF_DIR\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token variable\"}},[t._v(\"${HADOOP_CONF_DIR\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":-\")]),t._v(\"$PWD}\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#Hadoop配置文件的存放目录\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"核心配置文件 core-site.xml\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- fs.defaultFS: 默认文件系统 hdfs  --\\x3e\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"fs.defaultFS\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hdfs://hdp-01:9000\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" \\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"HDFS配置文件hdfs-site.xml\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- namenode 地址 --\\x3e\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.namenode.http-address\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hdp-01:50070\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- secondary namenode 地址 --\\x3e\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.namenode.http-address\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hdp-01:50090\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- 指定 namenode 软件存储元数据的本地目录 格式化节点时会自动生成--\\x3e\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.namenode.name.dir\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"/root/hdpdata/name\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- 指定 datanode 软件存储文件块的本地目录 格式化节点时会自动生成   --\\x3e\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.datanode.data.dir\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"/root/hdpdata/data\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" \\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"配置datanode集群节点文件 slaves\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-shell extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[t._v(\"hdp-01\\nhdp-02\\nhdp-03\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"配置好以上信息后，我们就可以将hadoop的包分发给其他的节点了\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-shell extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"scp\")]),t._v(\" -r hadoop-2.x.x root@hdp02:\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"目标路径\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"启动集群\")]),t._v(\" \"),n(\"p\",[t._v(\"在主节点上运行\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"hadoop namenode -format\\n\")])])]),n(\"p\",[t._v(\"运行完成后，节点会自动生成刚刚配置的工作目录\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"start-dfs.sh\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"浏览器输入http://x.x.x.x:50070查看集群运行情况\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(283),alt:\"namenode\"}})])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"最终服务器分布：\")]),t._v(\" \"),n(\"table\",[n(\"thead\",[n(\"tr\",[n(\"th\",[t._v(\"服务器地址\")]),t._v(\" \"),n(\"th\",[t._v(\"端口\")]),t._v(\" \"),n(\"th\",[t._v(\"服务\")])])]),t._v(\" \"),n(\"tbody\",[n(\"tr\",[n(\"td\",[t._v(\"hdp-01\")]),t._v(\" \"),n(\"td\",[t._v(\"50070\"),n(\"br\"),t._v(\"50090\")]),t._v(\" \"),n(\"td\",[t._v(\"namenode\"),n(\"br\"),t._v(\"secondaryNamenode\"),n(\"br\"),t._v(\"datanode\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"hdp-02\")]),t._v(\" \"),n(\"td\",[t._v(\"50010/50075\")]),t._v(\" \"),n(\"td\",[t._v(\"datanode\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"hdp-03\")]),t._v(\" \"),n(\"td\",[t._v(\"50010/50075\")]),t._v(\" \"),n(\"td\",[t._v(\"datanode\")])])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"问题总结\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"防火墙设置\\n为了防止发生一些奇怪的错误，请务必关闭所有节点的防火墙，他可能会导致浏览器无法获取集群信息和文件上传集群失败\")]),t._v(\" \"),n(\"li\",[t._v(\"hosts文件配置和主机名\\n因为这是完全分布式的集群，所以配置hosts文件至关重要，不然你的私钥配置和以后节点的格式化都会出错，他将会提示你无法解析主机名\")]),t._v(\" \"),n(\"li\",[t._v(\"请在关闭所有HDFS服务后在执行\"),n(\"code\",[t._v(\"-format\")]),t._v(\"格式化命令\\n如果存在节点未关闭，而你运行了格式化命令，这可能导致该节点与其他节点的目录ID不一致，从而导致“网络分区”问题\")])])])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"hdfs-体系架构\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-体系架构\"}},[t._v(\"#\")]),t._v(\" HDFS 体系架构\")]),t._v(\" \"),n(\"p\",[n(\"a\",{attrs:{href:\"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"官方文档\"),n(\"OutboundLink\")],1)]),t._v(\" \"),n(\"p\",[t._v(\"HDFS 是一种能够运行在商业硬件上的分布式文件系统，与目前市面上文件系统有很多相似之处，但是又是不同的软件系统。在HDFS中，使用的架构主从架构（Active|Standby），针对的是\"),n(\"strong\",[t._v(\"NameNode\")]),t._v(\"的高可用。\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"NameNode\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"存储文件的元数据如\"),n(\"code\",[t._v(\"文件名、文件目录结构、文件属性(生成时间、福本数、文件权限) ，以及每个文件的块列表和块所在的DataNode映射\")])]),t._v(\" \"),n(\"li\",[t._v(\"负责管理 DataNode\")]),t._v(\" \"),n(\"li\",[t._v(\"控制外界客户端对文件系统的访问。\")]),t._v(\" \"),n(\"li\",[t._v(\"NameNode会启动一个（或者说一个集群中，只有一个NameNode节点运行）\")])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"**DataNode **\\n在本地文件系统存储文件块数据，以及块数据的校验和，负责响应客户端对块的读写请求。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"Block\")]),t._v(\"\\n数据块，是对文件拆分的最小单元（默认情况下一个块大小128MB，每个数据块有3个副本）\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"rack\")])]),t._v(\" \"),n(\"p\",[t._v(\"机架，使用机架对存储节点进行物理编排，用于优化存储和计算\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(284),alt:\"img\"}})])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"什么是block块\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#什么是block块\"}},[t._v(\"#\")]),t._v(\" 什么是Block块\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"概述\")]),t._v(\" \"),n(\"p\",[t._v(\"hadoop集群中文件的存储都是以块的形式存储在hdfs中，且一般默认都会有三个备份分别存储在集中中不同的datanode节点上。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"默认值\")]),t._v(\" \"),n(\"p\",[t._v(\"从2.7.3版本开始 block size 默认值大小为 128M，之前版本默认值为64M\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"如何修改 block 块的大小？\")]),t._v(\" \"),n(\"p\",[t._v(\"可以通过修改 hdfs-site.xml中的 dfs.blocksize 对应的值\")]),t._v(\" \"),n(\"p\",[t._v(\"注意: 在修改HDFS的数据块大小时，首先停掉集群hadoop的运行，修改完毕后重新启动。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"block块的大小设置规则\")]),t._v(\" \"),n(\"p\",[t._v(\"在实际应用中，hdfs block块的大小设置为多少合适呢？为什么有的是64M，有个的是128M、256M、512M呢？\")]),t._v(\" \"),n(\"p\",[t._v(\"首先我们先来了解几个概念：\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[t._v(\"寻址时间：HDFS 中找到目标文件block所花费的时间。\")]),t._v(\" \"),n(\"li\",[t._v(\"原理：文件块越大，寻址时间越短，但磁盘传输时间越长；文件块越小，寻址时间越长，但磁盘传输时间越短。\")])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"block不能设置过大，也不能设置过小\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[t._v(\"如果设置过大，一方面从磁盘传输数据的时间会明显大于寻址时间，导致程序处理这块数据时，变得非常慢；另一方面，MapReduce 中map任务通常一次只处理一个块中的数据，如果过大运行速度也会很慢。\")]),t._v(\" \"),n(\"li\",[t._v(\"如果设置过小，一方面存放大小小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；另一方面块过小，寻址时间增长，导致程序一直在找block的开始位置。因此，块适当设置大一些，减少寻址时间，那么传输一个有多个块组成的文件时间\"),n(\"strong\",[t._v(\"主要取决于磁盘的传输速度\")]),t._v(\"。\")])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"设置多大合适？\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[n(\"p\",[t._v(\"HDFS中平均寻址时间大概为 10ms；\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"经过大量的测试发现，寻址时间为传输时间的1%时，为最佳状态，所以最佳的传输时间为：\")]),t._v(\" \"),n(\"p\",[t._v(\"10ms/0.01 = 1000s = 1s\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"目前磁盘的传输普遍为100MB/s，最佳block大小计算：\")]),t._v(\" \"),n(\"p\",[t._v(\"100MB/s*1s=100MB\")]),t._v(\" \"),n(\"p\",[t._v(\"所以我们设置block大小为 128MB。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"实际中，磁盘的传输速率为 200MB/s时，一般设定block块大小为256MB；磁盘传输速率为400MB/s时，一般设定block大小为512MB.\")])])])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"什么是机架感知？\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#什么是机架感知？\"}},[t._v(\"#\")]),t._v(\" 什么是机架感知？\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"背景\")]),t._v(\" \"),n(\"p\",[t._v(\"分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好多个机架，由多个机架上的机器共同组成分布式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机网络带宽的限制。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"存储策略\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"Hadoop在设计时考虑到数据的安全和高效，数据文件默认在HDFS上存放三份，存储策略为：\")])]),t._v(\" \"),n(\"p\",[t._v(\"第一个block副本放在客户端所在的数据节点上（如果客户端不在集群范围内，则从整个集群中随机选择一个合适的数据节点来存放）。\")]),t._v(\" \"),n(\"p\",[t._v(\"第二个副本放置在与第一个副本所在节点相同的机架内的其它数据节点上\")]),t._v(\" \"),n(\"p\",[t._v(\"第三个副本放置在不同机架节点上\")]),t._v(\" \"),n(\"p\",[t._v(\"如果副本数  >= 4时：\")]),t._v(\" \"),n(\"p\",[t._v(\"前三个副本按照上面原则存放，从第四个副本开始，随机选取dataNode节点存放（每个节点只保留一份副本，每个rack不超过两个副本）\")]),t._v(\" \"),n(\"p\",[t._v(\"这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上获取数据的速度快；\")]),t._v(\" \"),n(\"p\",[t._v(\"同时，如果整个机架的网络出现异常，也能保障可以在其它机架节点上找到数据。\")]),t._v(\" \"),n(\"p\",[t._v(\"为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。\")]),t._v(\" \"),n(\"p\",[t._v(\"如果在读取程序的同一个机架上有一个副本，那么就读取该副本。\")]),t._v(\" \"),n(\"p\",[t._v(\"如果在一个HDFS集群跨越多个数据中心，那么客户端也将读取本地数据中心的副本。\")]),t._v(\" \"),n(\"p\",[t._v(\"那么Hadoop是如何确定任意两个节点为于同一个机架，还是跨机架的呢？答案就是\"),n(\"strong\",[t._v(\"机架感知\")]),t._v(\"。\")]),t._v(\" \"),n(\"p\",[t._v('默认情况下，hadoop的机架感知是没有被开启的。所有机器hadoop都默认同一个机架下，名为“、default-rack\",这种情况下，任何一台 datanode 机器，不管物理上是否属于同一个机架，都会被认为在同一个机架下，此时，就很容易出现增添机架间网络负载的情况。因为此时hadoop集群的HDFS在选机器的时候，是随机选择的，也就是说，很可能在写数据的时候，hadoop将第一块数据的block1写在 rack1上，然后随机的选择将block2写入到rack2上。')]),t._v(\" \"),n(\"p\",[t._v(\"此时，两个rack之间产生了数据传输的流量，再接下来，在随机的情况下，又将block3重新又写回到rack2，此时，两个rack之间又产生了一次数据流量。\")]),t._v(\" \"),n(\"p\",[t._v(\"在job处理的数据量非常大的时候，或者hadoop推送的数据量非常大的时候，这种情况会造成rack之间的网络流量成倍的上升，成为性能的瓶颈，进而影响作业的性能甚至于整个集群的服务。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"配置\")]),t._v(\" \"),n(\"p\",[t._v(\"具体配置可参考：https://blog.csdn.net/zhongqi2513/article/details/73695229\")])])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"hdfs-核心工作原理\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-核心工作原理\"}},[t._v(\"#\")]),t._v(\" HDFS 核心工作原理\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"元数据管理\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#元数据管理\"}},[t._v(\"#\")]),t._v(\" 元数据管理\")]),t._v(\" \"),n(\"p\",[t._v(\"NameNode对数据的管理采用三种存储形式：\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"内存元数据（NameSystem）\")]),t._v(\" \"),n(\"li\",[t._v(\"磁盘元数据镜像文件\")]),t._v(\" \"),n(\"li\",[t._v(\"数据操作日志文件（可通过日志运算出元数据）\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"元数据存储机制\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#元数据存储机制\"}},[t._v(\"#\")]),t._v(\" 元数据存储机制\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"内存中有个一份完整的元数据（内存meta.data）\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"磁盘有一个”准完整“的元数据镜像（fsimage）文件（在namenode的工作目录中）\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"用于衔接内存meta.data和持久化元数据镜像fsimage之间的操作日志（edits文件）注：当客户端对hdfs中的文件进行新增或修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会被更新到内存meta.data中\")])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"secondarynamenode-检查点\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#secondarynamenode-检查点\"}},[t._v(\"#\")]),t._v(\" SecondaryNameNode 检查点\")]),t._v(\" \"),n(\"p\",[t._v(\"NameNode 职责是管理元数据信息，DataNode的职责是负责数据具体存储，那么 SecondaryNameNode 的作用是什么？它为什么会出现在 HDFS 中。从它的名字上看，它给人的感觉就像是 NameNode的备份。但实际上却不是。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"HDFS 集群运行一段时间后，可能会出现的问题：\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"edits 文件会变的很大，怎么去管理这个文件是一个挑战。\")]),t._v(\" \"),n(\"li\",[t._v(\"NameNode 重启会花费很长时间，因为有很多改动要合并到 fsimage 文件上。\")]),t._v(\" \"),n(\"li\",[t._v(\"如果 NameNode 挂掉了，那就丢失了一些改动。因为此时的 fsimage 文件非常旧。\")])]),t._v(\" \"),n(\"p\",[t._v(\"因此为了克服这个问题，我们需要一个易于管理的机制来帮助我们\"),n(\"code\",[t._v(\"减少 edits 文件的大小和得到一个最新的fsimage文件\")]),t._v(\"，这样也会减少在 NameNode 上的压力。这跟windows的恢复点是非常像的，windows的恢复点机制允许我们对OS进行快照，这样当系统发生问题时，我们能够回滚到最新的一次恢复点上。\")]),t._v(\" \"),n(\"p\",[t._v(\"SecondaryNameNode 就是来帮助我们解决上述问题的，它的职责是合并 NameNode 的 edits 到 fsimage文件中。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"Checkpoint\")])]),t._v(\" \"),n(\"p\",[t._v(\"每达到触发条件，会由 SecondaryNameNode 将 NameNode 上积累的所有 edits 和一个最新的 fsimage 下载到本地，并加载到内存进行 merge，而这个过程就称为：\"),n(\"strong\",[t._v(\"checkpoint\")])]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(285),alt:\"img\"}})]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"详细步骤\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"NameNode 管理着元数据信息，其中有两类持久化元数据文件：edits 操作日志文件和 fsImage 元数据镜像文件。新的操作日志不会立即与 fsimage 进行合并，也不会刷到 NameNode 内存中，而是会先写到 edits 中（因为合并需要消耗大量的资源）。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"有dfs.namenode.checkpoint.period和dfs.namenode.checkpoint.txns 两个配置，只要达到这两个条件任何一个，secondarynamenode就会执行checkpoint的操作。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"当触发 checkpoint 操作时，SecondaryNameNode 告诉 NameNode 滚动到它的 edits_inprogress 文件，这样新来的write操作就会放到一个这个新的 edits文件中。同时，Secondary 通过HTTP GET方式从 NameNode 获取到最新的 fsimage 和 edits 文件\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"SecondaryNameNode 将下载下来的 fsimage 载入到内存，然后一条一条的执行 edits 文件中的各项操作，使得内存中的 fsimage 保持最新，这个过程就是 edits和fsimage文件合并，生成一个新的 fsimage文件，即上图的 fsimage.ckpt 文件。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"SecondaryNameNode 将新生成的 fsimage.ckpt文件复制到NameNode节点。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"在 NameNode 节点的 edits.new 文件和 fsimage.ckpt文件会替换掉原来的 edits 文件 和 fsimage 文件，至此刚好是一个轮回。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"等待下一次checkpoint触发SecondaryNameNode进行工作，一直这样循环操作。\")])])]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"补充：\")])]),t._v(\" \"),n(\"p\",[t._v(\"NameNode 和 Secondary NameNode 的工作目录存储结构是完全相同的，所以，当NameNode故障退出需要重新恢复时，可以从Secondary NameNode的工作目录中将fsimage拷贝到NameNode的工作目录，以恢复NameNode的元数据。\")]),t._v(\" \"),n(\"p\",[t._v(\"chepoint检查时间参数设置：\")]),t._v(\" \"),n(\"p\",[t._v(\"（1）通常情况下，SecondaryNameNode每个一小时执行一次。\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[t._v(\"# hdfs-default.xml\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.namenode.checkpoint.period\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"3600\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"（2）一分钟检查一次操作次数，当操作次数达到一百万时，SecondaryNameNode执行一次。\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.namenode.checkpoint.txns\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"1000000\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"description\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"操作动作次数\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"description\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.namenode.checkpoint.check.period\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"60\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"description\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" 1分钟检查一次操作次数\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"description\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"从上面的描述我们可以看出，SecondaryNameNode 根本不是 NameNode 的一个热备，其只是将 fsimage 和 edits 合并。其拥有的 fsimage 不会最新的，因为它从 NameNode 下载 fsimage 和 edits 文件时候，新的更新操作已经写到 edit.new 文件中去了。而这些更新在 SecondaryNameNode 是没有同步到的。当然，如果 NameNode 中的 fsimage 真的出问题了，还是可以用 SecondaryNameNode 中的 fsimage 替换一下 NameNode 上的 fimage，虽然已经不是最新的 fsimage，但是我们可以将损失减少到最小！\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"hdfs-写数据流程\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-写数据流程\"}},[t._v(\"#\")]),t._v(\" HDFS 写数据流程\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(286),alt:\"\"}})]),t._v(\" \"),n(\"h3\",{attrs:{id:\"hdfs-读数据流程\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-读数据流程\"}},[t._v(\"#\")]),t._v(\" HDFS 读数据流程\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(287),alt:\"\"}})]),t._v(\" \"),n(\"h3\",{attrs:{id:\"安全模式\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#安全模式\"}},[t._v(\"#\")]),t._v(\" 安全模式\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"安全模式的作用\")])]),t._v(\" \"),n(\"p\",[t._v(\"Hadoop 的安全模式即只读模式，是指当前系统中数据的副本数比较少，在该阶段要对数据块进行复制操作，不允许外界对数据库进行修改和删除等操作。处于安全模式的NameNode是不会进行数据块的复制的。\")]),t._v(\" \"),n(\"p\",[t._v(\"NameNode 启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内中成功建立文件系统元数据的映像，则创建一个新的 fsimage 文件（这个操作不需要SecondaryNameNode）和一个空的编辑文件（edits_inprogress...）。此时，NameNode 开始监听 RPC 和 HTTP 请求。但此时，NameNode 运行在安全模式下，对于客户端来说是只读的。\")]),t._v(\" \"),n(\"p\",[t._v(\"处于安全模式的NameNode是不会进行数据块的复制的。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"何时进入安全模式\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"NameNode在启动的时候首先进入安全模式\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"满足最小复本数要求的数据块比例达不到dfs.safemode.threshold.pct\")]),t._v(\" \"),n(\"p\",[t._v(\"如果datanode丢失的block达到一定的比例（1-dfs.safemode.threshold.pct），则系统会一直处于安全模式状态即只读状态。dfs.safemode.threshold.pct（缺省值0.999f）表示HDFS启动的时候，如果DataNode上报的block个数达到了元数据记录的block个数的0.999倍才可以离开安全模式，否则一直是这种只读模式。如果设为1则HDFS永远是处于SafeMode。\")])])]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"何时退出安全模式\")])]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"如果满足“最小复本条件”namenode会在30秒之后退出安全模式。所谓的最小复本条件指的是文件系统中有99.9%的块满足最小复本级别（默认值是1，由dfs.replication.min属性设置）。\")]),t._v(\" \"),n(\"li\",[t._v(\"手动退出\")])]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"安全模式的配置\")])]),t._v(\" \"),n(\"p\",[t._v(\"https://www.iteblog.com/archives/977.html\\nhttps://www.cnblogs.com/admln/p/5821983.html\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[t._v(\"dfs.replication：设置数据块应该被复制的份数；\\ndfs.replication.min：所规定的数据块副本的最小份数；\\ndfs.replication.max：所规定的数据块副本的最大份数；\\ndfs.safemode.threshold.pct：指定应有多少比例的数据块满足最小副本数要求。\\n  (1)当小于这个比例， 那就将系统切换成安全模式，对数据块进行复制；\\n  (2)当大于该比例时，就离开安全模式，说明系统有足够的数据块副本数，可以对外提供服务。\\n  (3)小于等于0意味不进入安全模式，大于1意味一直处于安全模式。\\n\")])])]),n(\"p\",[t._v(\"副本数按dfs.replication设置，如果有失效节点导致某数据块副本数降低，当低于dfs.replication.min后，系统再在其他节点处复制新的副本。如果该数据块的副本经常丢失，导致在环境中太多的节点处复制了超过dfs.replication.max的副本数，那么就不再复制了。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"手动操作安全模式\")])]),t._v(\" \"),n(\"p\",[t._v(\"①查看namenode是否处于安全模式：hadoop dfsadmin –safemode get\\n②执行某条命令前namenode先退出安全模式：hadoop dfsadmin –safe wait\\n③进入安全模式：hadoop dfsadmin –safemode enter\\n④离开安全模式：hadoop dfsadmin –safemode leave\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"从一个-hdfs-edits-文件看到的一些问题\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#从一个-hdfs-edits-文件看到的一些问题\"}},[t._v(\"#\")]),t._v(\" 从一个 HDFS edits 文件看到的一些问题\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"-rw-r--r-- 1 root root      42 Mar 16 11:28 edits_0000000000000010479-0000000000000010480\\n-rw-r--r-- 1 root root      42 Mar 16 11:29 edits_0000000000000010481-0000000000000010482\\n-rw-r--r-- 1 root root      42 Mar 16 11:30 edits_0000000000000010483-0000000000000010484\\n-rw-r--r-- 1 root root      42 Mar 16 11:31 edits_0000000000000010485-0000000000000010486\\n-rw-r--r-- 1 root root      42 Mar 16 11:32 edits_0000000000000010487-0000000000000010488\\n-rw-r--r-- 1 root root      42 Mar 16 11:33 edits_0000000000000010489-0000000000000010490\\n-rw-r--r-- 1 root root      42 Mar 16 11:34 edits_0000000000000010491-0000000000000010492\\n-rw-r--r-- 1 root root      42 Mar 16 11:35 edits_0000000000000010493-0000000000000010494\\n-rw-r--r-- 1 root root      42 Mar 16 11:36 edits_0000000000000010495-0000000000000010496\\n-rw-r--r-- 1 root root      42 Mar 16 11:37 edits_0000000000000010497-0000000000000010498\\n-rw-r--r-- 1 root root      42 Mar 16 11:38 edits_0000000000000010499-0000000000000010500\\n-rw-r--r-- 1 root root      42 Mar 16 11:39 edits_0000000000000010501-0000000000000010502\\n-rw-r--r-- 1 root root      42 Mar 16 11:40 edits_0000000000000010503-0000000000000010504\\n-rw-r--r-- 1 root root      42 Mar 16 11:41 edits_0000000000000010505-0000000000000010506\\n-rw-r--r-- 1 root root      42 Mar 16 11:42 edits_0000000000000010507-0000000000000010508\\n-rw-r--r-- 1 root root      42 Mar 16 11:43 edits_0000000000000010509-0000000000000010510\\n-rw-r--r-- 1 root root      42 Mar 16 11:44 edits_0000000000000010511-0000000000000010512\\n-rw-r--r-- 1 root root      42 Mar 16 11:45 edits_0000000000000010513-0000000000000010514\\n-rw-r--r-- 1 root root      42 Mar 16 11:46 edits_0000000000000010515-0000000000000010516\\n-rw-r--r-- 1 root root      42 Mar 16 11:47 edits_0000000000000010517-0000000000000010518\\n-rw-r--r-- 1 root root      42 Mar 16 11:48 edits_0000000000000010519-0000000000000010520\\n-rw-r--r-- 1 root root      42 Mar 16 11:49 edits_0000000000000010521-0000000000000010522\\n-rw-r--r-- 1 root root      42 Mar 16 11:50 edits_0000000000000010523-0000000000000010524\\n-rw-r--r-- 1 root root      42 Mar 16 11:51 edits_0000000000000010525-0000000000000010526\\n-rw-r--r-- 1 root root      42 Mar 16 11:52 edits_0000000000000010527-0000000000000010528\\n-rw-r--r-- 1 root root      42 Mar 16 11:53 edits_0000000000000010529-0000000000000010530\\n-rw-r--r-- 1 root root      42 Mar 16 11:54 edits_0000000000000010531-0000000000000010532\\n-rw-r--r-- 1 root root      42 Mar 16 11:55 edits_0000000000000010533-0000000000000010534\\n-rw-r--r-- 1 root root      42 Mar 16 11:56 edits_0000000000000010535-0000000000000010536\\n-rw-r--r-- 1 root root 1048576 Mar 16 11:56 edits_inprogress_0000000000000010537\\n-rw-r--r-- 1 root root     322 Mar 12 20:05 fsimage_0000000000000000000\\n-rw-r--r-- 1 root root      62 Mar 12 20:05 fsimage_0000000000000000000.md5\\n-rw-r--r-- 1 root root       6 Mar 16 11:56 seen_txid\\n-rw-r--r-- 1 root root     214 Mar 12 20:05 VERSION\\n[root@thtf-01 current]# \\n\")])])]),n(\"ol\",[n(\"li\",[n(\"p\",[t._v(\"为什么会有这么多 edits文件？\")]),t._v(\" \"),n(\"p\",[t._v(\"这些edits 文件创建的时间都是间隔1个小时，这是因为每隔1小时，SecondaryNameNode 都会对 edits 和 fsimage 进行一次合并，合并之后，旧的 edits 并不会被删除，依旧被保留。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"edits_inprogress… 文件作用？\")]),t._v(\" \"),n(\"p\",[t._v(\"最新的一个edtis文件，用来记录用户的上传、删除操作。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"那我们看 fsimage 只保留两个文件。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"fsimage… .md5\")]),t._v(\"： 是对应文件的md5值文件，用来保证 fsimage的一致性的。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"seen_txid\")]),t._v(\"：里面保存了最新的一个 edits 文件的名字 最后的三位：107.\")]),t._v(\" \"),n(\"p\",[t._v(\"NameNode 格式化后 第一次启动时，会创建一个 edits 和 fsimage 文件，后面再次启动时，直接加载 每个 edits 和最新的 fsimage 文件。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"为什么要每个 edits 都加载呢？fsimage不是已经保存之前 edits日志当中转换后的 元数据了么？\")]),t._v(\" \"),n(\"p\",[t._v(\"这是为了再做一次校验的工作，确保加载到内存当中的元数据是可靠的。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"Edits 文件是用来做什么的？为什么要保留这么多 edtis文件，像 fsimage一样保存一两个文件不就可以了么？\")]),t._v(\" \"),n(\"p\",[t._v(\"1）edits是用来存放 用户的 上传、删除请求的日志的。当用户上传成功，这条记录会保存到 内存元数据当中。也就是说，edits是作为一个中间文件。\")]),t._v(\" \"),n(\"p\",[t._v(\"2）当 secondarynamenode 对 edtis 和 fsimage 进行合并时，会创建一个新的 edits 文件，以接收在合并期间 来的新的 用户上传请求。所以，每次触发 Secondaryname 的 checkpoint（也就是对edits和 fsimage 合并），都会产生一个新的 edtis 文件。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"旧的 edtis 文件一直会保留，会不会有一天 edits把空间沾满了啊？还有 fsimage，它保留了所有 block块的信息，总有一天会很大吧？\")]),t._v(\" \"),n(\"p\",[t._v(\"这个问题其实我们不用担心，因为 edtis 和 fsimage 只是保留了 每个文件的位置等的信息，这些信息经过很长时间也不会占用多少空间的。\")]),t._v(\" \"),n(\"p\",[t._v(\"但也会遇到小文件太多的问题，比如下面场景：\")]),t._v(\" \"),n(\"table\",[n(\"thead\",[n(\"tr\",[n(\"th\",[t._v(\"文件\")]),t._v(\" \"),n(\"th\",[t._v(\"NameNode内存占用\")]),t._v(\" \"),n(\"th\",[t._v(\"DataNode磁盘占用\")])])]),t._v(\" \"),n(\"tbody\",[n(\"tr\",[n(\"td\",[t._v(\"128MB\")]),t._v(\" \"),n(\"td\",[t._v(\"一个block块的元数据信息 1KB\")]),t._v(\" \"),n(\"td\",[t._v(\"128MB\")])]),t._v(\" \"),n(\"tr\",[n(\"td\",[t._v(\"128MB*10000个文件\")]),t._v(\" \"),n(\"td\",[t._v(\"10000block块元数据信息：10MB\")]),t._v(\" \"),n(\"td\",[t._v(\"128MB\")])])])]),t._v(\" \"),n(\"p\",[t._v(\"HDFS最初是为流式访问大文件开发的，如果访问大量小文件，需要不断的从一个datanode跳到另一个datanode，严重影响性能。\")]),t._v(\" \"),n(\"p\",[t._v(\"引申出问题：\")]),t._v(\" \"),n(\"p\",[t._v(\"（1）NameNode所在的物理节点内存应该多给一点\")]),t._v(\" \"),n(\"p\",[t._v(\"（2）理论上hdfs是可以无限扩充的，因此可以在横向上扩展无数个节点。但是因为NameNode实际只能有一个运行，所以hdfs的上限容量受制于NameNode的内存上限容量。\")])])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"hdfs-命令行客户端基本操作\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-命令行客户端基本操作\"}},[t._v(\"#\")]),t._v(\" HDFS 命令行客户端基本操作\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"客户端理解\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#客户端理解\"}},[t._v(\"#\")]),t._v(\" 客户端理解\")]),t._v(\" \"),n(\"p\",[t._v(\"HDFS 的客户端有多种形式：\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"网页形式\")]),t._v(\" \"),n(\"li\",[t._v(\"命令行形式\")])]),t._v(\" \"),n(\"p\",[t._v(\"客户端在哪里运行，没有约束，只要运行客户端的机器能够跟 HDFS 集群联网。\")]),t._v(\" \"),n(\"blockquote\",[n(\"p\",[t._v(\"注意：文件的切块大小和存储的福本数量，都是由客户端决定的！\")])]),t._v(\" \"),n(\"p\",[t._v(\"所谓客户端决定，是通过配置参数类定的。HDFS 的客户端会读取以下两个参数，来决定切块大小和副本数量：\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"切块大小参数：dfs.blocksize\")]),t._v(\" \"),n(\"li\",[t._v(\"副本数量的参数：dfs.replication\")])]),t._v(\" \"),n(\"p\",[t._v(\"上面两个参数在 hdfs-site.xml 中配置：\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.blocksize\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"128m\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.replication\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"3\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"hdfs-命令行客户端的常用操作命令\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-命令行客户端的常用操作命令\"}},[t._v(\"#\")]),t._v(\" HDFS 命令行客户端的常用操作命令\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[n(\"p\",[t._v(\"查看 HDFS 目录信息\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-shell extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[t._v(\"hadoop fs -ls /hdfs目录\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"在 HDFS 中创建文件夹\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -mkdir -p /aa/bb/cc\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"移动 HDFS 中的文件（改名）\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -mv /hdfs路径\\t/hdfs另一个路径\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"上传文件到 HDFS 中\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -put /本地文件 /hdfs目录\\nhadoop fs -copyFromLocal/本地文件\\t/hdfs目录   \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"## copyFromLocak == put\")]),t._v(\"\\nhadoop fs -moveFromLocal/本地文件\\t/hdfs目录\\t\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"## 从本地移动到hdfs\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"下载文件到客户端本地\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -get /hdfs中的文件\\t/本地目录\\nhadoop fs -copyToLocal/hdfs中的文件\\t/本地目录\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"## copyToLocal == get\")]),t._v(\"\\nhadoop fs -moveToLocal/hdfs中的文件 /本地目录\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"## 从 hdfs 中移动到本地\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"删除 HDFS 中的文件或文件夹\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"rm\")]),t._v(\" -r /hdfs目录或文件\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"修改文件的权限\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -chown user:group /hdfs文件\\nhadoop fs -chmod \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"777\")]),t._v(\" /hdfs文件\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"追加内容到已有文件\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -appendToFile /本地文件\\t/hdfs文件\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"显示文本文件内容\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-sh extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-sh\"}},[n(\"code\",[t._v(\"hadoop fs -cat /hdfs文件\\nhadoop fs -tail /hdfs文件 \\n\")])])])])]),t._v(\" \"),n(\"h2\",{attrs:{id:\"hdfs-的-javaapi\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs-的-javaapi\"}},[t._v(\"#\")]),t._v(\" HDFS 的 JavaAPI\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"开发环境准备\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#开发环境准备\"}},[t._v(\"#\")]),t._v(\" 开发环境准备\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[t._v(\"在本地解压Hadoop安装包\")]),t._v(\" \"),n(\"li\",[t._v(\"在环境变量中配置：HADOOP_HOME\")]),t._v(\" \"),n(\"li\",[t._v(\"创建 SpringBoot 工程,引入 Hadoop 依赖包\")])]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- hadoop-hdfs --\\x3e\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"dependency\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"groupId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"org.apache.hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"groupId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"artifactId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hadoop-hdfs\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"artifactId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"version\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"2.10.0\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"version\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"dependency\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- hadoop-common --\\x3e\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"dependency\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"groupId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"org.apache.hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"groupId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"artifactId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hadoop-common\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"artifactId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"version\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"2.10.0\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"version\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"dependency\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"文件上传\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#文件上传\"}},[t._v(\"#\")]),t._v(\" 文件上传\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"conf\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FSDataOutputStream\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"junit\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"After\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"junit\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Before\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"junit\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Test\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputStream\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileNotFoundException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"App\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),t._v(\" configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Before\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getClient\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HADOOP_USER_NAME\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"root\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        configuration \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 手动设置连接信息\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v('//configuration.set(\"fs.defaultFS\", \"\")')]),t._v(\"\\n        configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"core-site.xml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"hdfs-site.xml \"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        fs \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"newInstance\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@After\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"close\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"close\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"upload1\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"copyFromLocalFile\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/本地文件\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"upload2\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Exception\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputStream\")]),t._v(\" inputStream \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputStream\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/本地文件\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FSDataOutputStream\")]),t._v(\" outputStream \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"create\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/a.txt\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"copyBytes\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputStream\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" outputStream\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1024\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[n(\"strong\",[t._v(\"权限不足解决方案\")])]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"security\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"AccessControlException\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Permission\")]),t._v(\" denied\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" user\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"wolf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" access\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"WRITE\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" inode\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\"root\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\"supergroup\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\"drwxr\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"-\")]),t._v(\"xr\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"-\")]),t._v(\"x\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"\\n\")])])]),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"方案一：\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HADOOP_USER_NAME\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"root\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"方案二：\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-shell extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[t._v(\"java -jar xxxx -DHADOOP_USER_NAME\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\"root\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"方案三：\")]),t._v(\" \"),n(\"blockquote\",[n(\"p\",[t._v(\"将权限检查机制关闭：ect/hadoop/hdfs-site.xml:\")])]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"dfs.permissions.enabled\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"<value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"false\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"下载文件\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#下载文件\"}},[t._v(\"#\")]),t._v(\" 下载文件\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"download1\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"copyToLocalFile\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/hdfs文件\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/本地目录\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"download2\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputStream\")]),t._v(\" inputStream \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputStream\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"hdfs文件\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FSDataOutputStream\")]),t._v(\" outputStream \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"create\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/本地目录\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"copyBytes\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"inputStream\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" outputStream\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1024\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"创建文件夹\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#创建文件夹\"}},[t._v(\"#\")]),t._v(\" 创建文件夹\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"create\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mkdirs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/a/b\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"删除文件\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#删除文件\"}},[t._v(\"#\")]),t._v(\" 删除文件\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"delete\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 递归删除\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"boolean\")]),t._v(\" delete \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"delete\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/a\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"递归查询目录下所有文件列表\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#递归查询目录下所有文件列表\"}},[t._v(\"#\")]),t._v(\" 递归查询目录下所有文件列表\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"listFile\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"RemoteIterator\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LocatedFileStatus\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" remoteIterator \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"listFiles\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"while\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"remoteIterator\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"hasNext\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LocatedFileStatus\")]),t._v(\" fileStatus \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" remoteIterator\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"next\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" name \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fileStatus\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getPath\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getName\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"out\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"println\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"name\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"判断文件是否存在\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#判断文件是否存在\"}},[t._v(\"#\")]),t._v(\" 判断文件是否存在\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"exist\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"boolean\")]),t._v(\" exists \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"exists\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/a.txt\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"out\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"println\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"exists\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"回收站\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#回收站\"}},[t._v(\"#\")]),t._v(\" 回收站\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Test\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"trash\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Trash\")]),t._v(\" trash \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Trash\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"boolean\")]),t._v(\" b \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" trash\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"moveToTrash\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/a.txt\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"out\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"println\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"b\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"hdfs客户端编程应用场景：数据采集\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs客户端编程应用场景：数据采集\"}},[t._v(\"#\")]),t._v(\" HDFS客户端编程应用场景：数据采集\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(288),alt:\"\"}})]),t._v(\" \"),n(\"h4\",{attrs:{id:\"需求描述\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#需求描述\"}},[t._v(\"#\")]),t._v(\" 需求描述\")]),t._v(\" \"),n(\"p\",[t._v(\"在业务系统服务器上，业务程序会不断生成业务日志（比如网站的访问日志）\")]),t._v(\" \"),n(\"p\",[t._v(\"业务日志是用log4j生成的，会不断的切出日志文件\")]),t._v(\" \"),n(\"p\",[t._v(\"需要定期（如：每小时）从业务服务器上的日志文件中，探测需要采集的日志文件（access.log)，发往HDFS\")]),t._v(\" \"),n(\"p\",[t._v(\"注意点：业务服务器可能有多台（hdfs上的文件名不能直接用日志服务器上的文件名）\")]),t._v(\" \"),n(\"p\",[t._v(\"当天采集的日志要放在hdfs的当天目录中\")]),t._v(\" \"),n(\"p\",[t._v(\"采集完成的日志文件，需要移动到日志服务器的一个备份目录中\")]),t._v(\" \"),n(\"p\",[t._v(\"定期检查（一小时检查一次）备份目录，将备份时长超过24小时的日志文件清除\")]),t._v(\" \"),n(\"h4\",{attrs:{id:\"数据采集设计\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#数据采集设计\"}},[t._v(\"#\")]),t._v(\" 数据采集设计\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"1、流程\")])]),t._v(\" \"),n(\"p\",[t._v(\"启动一个定时任务\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"定时探测日志源目录\")]),t._v(\" \"),n(\"li\",[t._v(\"获取需要采集的文件\")]),t._v(\" \"),n(\"li\",[t._v(\"移动这些文件到一个待上传的临时目录\")]),t._v(\" \"),n(\"li\",[t._v(\"遍历待上传目录中的各个文件，逐一传输到HDFS的目标路径，同时将传输完成的文件移动到备份目录\")])]),t._v(\" \"),n(\"p\",[t._v(\"启动一个定时任务\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"探测备份目录中的备份数据，检查是否已超出最长备份时长，如果超出，则删除\")])]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"2、规划各种路径\")])]),t._v(\" \"),n(\"p\",[t._v(\"日志源路径：/Users/wolf/logs/access\")]),t._v(\" \"),n(\"p\",[t._v(\"待上传临时目录：/Users/wolf/logs /tmp\")]),t._v(\" \"),n(\"p\",[t._v(\"备份目录：/Users/wolf/logs/backup/日期\")]),t._v(\" \"),n(\"p\",[t._v(\"HDFS存储路径：/logs/日期\")]),t._v(\" \"),n(\"p\",[t._v(\"HDFS文件前缀：access_log_\")]),t._v(\" \"),n(\"p\",[t._v(\"HDFS文件后缀：.log\")]),t._v(\" \"),n(\"h4\",{attrs:{id:\"代码实现\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#代码实现\"}},[t._v(\"#\")]),t._v(\" 代码实现\")]),t._v(\" \"),n(\"p\",[t._v(\"配置文件：collect.properties\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-properties extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-properties\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"LOG_SOURCE_DIR\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"/logs/access/\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"LOG_TMP_DIR\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"/logs/tmp/\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"LOG_BACKUP_BASE_DIR\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"/logs/backup/\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"LOG_BACKUP_TIMEOUT\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"24\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"HDFS_DEST_BASE_DIR\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"/logs\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"HDFS_FILE_PREFIX\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"access_log_\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"HDFS_FILE_SUFFIX\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\".log\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"配置常量池： Constants.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" LOG_SOURCE_DIR \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"LOG_SOURCE_DIR\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" LOG_TMP_DIR \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"LOG_TMP_DIR\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" LOG_BACKUP_BASE_DIR \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"LOG_BACKUP_BASE_DIR\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" LOG_BACKUP_TIMEOUT \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"LOG_BACKUP_TIMEOUT\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" HDFS_DEST_BASE_DIR \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HDFS_DEST_BASE_DIR\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" HDFS_FILE_PREFIX \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HDFS_FILE_PREFIX\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"final\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" HDFS_FILE_SUFFIX \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HDFS_FILE_SUFFIX\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"配置文件加载类：PropertyHolder.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"PropertyHolder\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"private\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),t._v(\" prop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProps\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"prop \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"==\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"synchronized\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"PropertyHolder\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"prop \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"==\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                    prop \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                    prop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"load\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"PropertyHolder\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getClassLoader\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getResourceAsStream\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"collect.properties\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" prop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\")])])]),n(\"p\",[t._v(\"日志收集task：CollectTask.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"commons\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"conf\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"slf4j\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Logger\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"slf4j\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LoggerFactory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"SimpleDateFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"*\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"CollectTask\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"extends\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"TimerTask\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"private\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Logger\")]),t._v(\" logger \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LoggerFactory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getLogger\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"CollectTask\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Override\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"run\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// - 定时探测日志源目录\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// - 获取需要采集的文件\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// - 移动这些文件到一个待上传的临时目录\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// - 遍历待上传目录中的各个文件，逐一传输到HDFS的目标路径，同时将传输完成的文件移动到备份目录\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 获取本地采集是的日期\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"SimpleDateFormat\")]),t._v(\" sdf \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"SimpleDateFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"yyyy-MM-dd-HH\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" date \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" sdf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"format\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Date\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 构造一个HDFS的客户端对象\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"try\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),t._v(\" props \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"PropertyHolder\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProps\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" srcDir  \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"LOG_SOURCE_DIR\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 列出日志源目录中需要采集的文件\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" listFiles \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" srcDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"listFiles\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" dir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" name\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"->\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" name\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"startsWith\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"access.log\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"listFiles\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"length \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"==\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                logger\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"info\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"探测到如下文件需要采集为空！\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n            logger\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"info\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"探测到如下文件需要采集：{}\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Arrays\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"toString\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"listFiles\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 将要采集的文件移动到待上传临时目录\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" tmpDir \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"LOG_TMP_DIR\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" file \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" listFiles\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"moveFileToDirectory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"file\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" tmpDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HADOOP_USER_NAME\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"root\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),t._v(\" configuration \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 手动设置连接信息\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v('//configuration.set(\"fs.defaultFS\", \"\")')]),t._v(\"\\n            configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"core-site.xml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"hdfs-site.xml \"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),t._v(\" fs \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"newInstance\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"configuration\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 检查HDFS的日期目录是否存在，不存在就创建\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),t._v(\" hdfsDatePath \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"HDFS_DEST_BASE_DIR\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" date\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!\")]),t._v(\"fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"exists\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"hdfsDatePath\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mkdirs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"hdfsDatePath\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" toUploadFiles \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tmpDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"listFiles\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" file \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" toUploadFiles\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),t._v(\" destPath \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"hdfsDatePath \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"HDFS_FILE_PREFIX\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" UUID\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"randomUUID\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"HDFS_FILE_SUFFIX\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 传输文件到HDFS并修改名称\")]),t._v(\"\\n                fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"copyFromLocalFile\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"file\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getAbsolutePath\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" destPath\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                logger\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"info\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"文件上传hdfs完成：{} --\\x3e {}\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" file\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getAbsolutePath\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" destPath\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" backupDir \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Constants\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"LOG_BACKUP_BASE_DIR\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" date \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"moveFileToDirectory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"file\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" backupDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                logger\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"info\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"文件备份完成：{} --\\x3e {}\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" file\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getAbsolutePath\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" backupDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n             \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"catch\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Exception\")]),t._v(\" e\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            e\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"printStackTrace\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"日志备份清理task：BackupCleanTask.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"commons\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"slf4j\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Logger\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"slf4j\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LoggerFactory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"SimpleDateFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"TimerTask\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"BackupCleanTask\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"extends\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"TimerTask\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"private\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Logger\")]),t._v(\" logger \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LoggerFactory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getLogger\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"BackupCleanTask\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"/**\\n     * The action to be performed by this timer task.\\n     */\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Override\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"run\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        logger\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"info\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"定时清理任务开始执行\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 获取本地采集是的日期\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"SimpleDateFormat\")]),t._v(\" sdf \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"SimpleDateFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"yyyy-MM-dd-HH\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"long\")]),t._v(\" now \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"currentTimeMillis\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"try\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 探测本地备份目录\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" backupBaseDir \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/Users/wolf/logs/backup/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" dateBackDir \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" backupBaseDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"listFiles\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 判断备份日期子目录是否已经超过24小时\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"File\")]),t._v(\" dir \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" dateBackDir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!\")]),t._v(\"dir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"isHidden\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"long\")]),t._v(\" time \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" sdf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"parse\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"dir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getName\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getTime\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"now \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"-\")]),t._v(\" time\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\">\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"24\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1000L\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileUtils\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"deleteDirectory\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"dir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                        logger\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"info\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"备份目录：{} 清理完毕！\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" dir\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getName\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"catch\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Exception\")]),t._v(\" e\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            e\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"printStackTrace\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"定时任务main函数：DataCollectMain.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Timer\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"DataCollectMain\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"main\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" args\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Timer\")]),t._v(\" timer \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Timer\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 定时数据收集\")]),t._v(\"\\n        timer\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"schedule\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"CollectTask\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1000L\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 定时清理历史数据备份\")]),t._v(\"\\n        timer\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"schedule\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"BackupCleanTask\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"60\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"*\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1000L\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"h3\",{attrs:{id:\"hdfs版的wordcount\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hdfs版的wordcount\"}},[t._v(\"#\")]),t._v(\" HDFS版的wordcount\")]),t._v(\" \"),n(\"p\",[t._v(\"HdfsWordCount.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"conf\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"*\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"BufferedReader\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"InputStreamReader\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"net\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"URI\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HashMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Set\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HdfsWordCount\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"main\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" args\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Exception\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 初始化\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),t._v(\" props \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Properties\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"load\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HdfsWordCount\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getClassLoader\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getResourceAsStream\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"job.properties\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Class\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"?\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" mapper_impl_class \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"forName\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"props\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"MAPPER_IMPL_CLASS\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Mapper\")]),t._v(\" mapper \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Mapper\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" mapper_impl_class\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"newInstance\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),t._v(\" context \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 去hdfs中读取文件：一次读一行\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),t._v(\" fs \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"get\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"URI\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"hdfs://10.10.50.189:9000\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"root\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"RemoteIterator\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LocatedFileStatus\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" iterator \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"listFiles\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/wordcount/input\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"false\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"while\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"iterator\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"hasNext\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LocatedFileStatus\")]),t._v(\" file \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" iterator\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"next\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FSDataInputStream\")]),t._v(\" in \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"open\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"file\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getPath\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"BufferedReader\")]),t._v(\" br \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"BufferedReader\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"InputStreamReader\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"in\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" line \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"while\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"line \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" br\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"readLine\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 调用一个方法对每一行进行业务处理\")]),t._v(\"\\n                mapper\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"line\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n            br\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"close\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            in\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"close\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HashMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" contextMap \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getContextMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),t._v(\" outPath \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/wordcount/output/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"!\")]),t._v(\"fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"exists\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"outPath\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"mkdirs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"outPath\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FSDataOutputStream\")]),t._v(\" out \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"create\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/wordcount/output/res.dat\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Set\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Entry\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" entrySet \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" contextMap\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"entrySet\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Entry\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" entry \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" entrySet\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            out\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"write\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"entry\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getKey\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"toString\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"\\\\t\"')]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" entry\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getValue\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"\\\\n\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getBytes\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n        out\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"close\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"close\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"out\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"println\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"数据统计完成！\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"job.properties\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-properties extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-properties\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token attr-name\"}},[t._v(\"MAPPER_IMPL_CLASS\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"=\")]),n(\"span\",{pre:!0,attrs:{class:\"token attr-value\"}},[t._v(\"com.pyy.wordcount.WordCountMapper\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"Mapper.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"interface\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Mapper\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" line\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n\")])])]),n(\"p\",[t._v(\"WordCountMapper.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"WordCountMapper\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"implements\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Mapper\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Override\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" line\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" words \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" line\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"split\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\" \"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" word \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" words\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),t._v(\" value \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"get\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"word\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"null\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"==\")]),t._v(\" value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"write\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"word\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"else\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n                \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"int\")]),t._v(\" v \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"int\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n                context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"write\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"word\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" v\"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"Context.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HashMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"private\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HashMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" contextMap \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HashMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"write\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),t._v(\" key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),t._v(\" value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        contextMap\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"put\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"get\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),t._v(\" key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" contextMap\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"get\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"HashMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Object\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getContextMap\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"return\")]),t._v(\" contextMap\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"统计文件：\")]),t._v(\" \"),n(\"p\",[t._v(\"a.txt:\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"hello tom\\nhello jim\\nhello jeca\\nhello kata\\nhello wolf\\n\")])])]),n(\"p\",[t._v(\"b.txt\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"hello tom\\nhello jim\\nhello jeca\\nhello kata\\nhello wolf\\n\")])])]),n(\"p\",[t._v(\"c.txt\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"hello tom\\nhello jim\\nhello jeca\\nhello kata\\nhello wolf\\n\")])])]),n(\"p\",[t._v(\"d.txt\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"hello tom\\nhello jim\\nhello jeca\\nhello kata\\nhello wolf\\n\")])])]),n(\"p\",[t._v(\"统计结果：\")]),t._v(\" \"),n(\"p\",[t._v(\"rst.dat\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"kata    4\\ntom     4\\nwolf    4\\njeca    4\\nhello   20\\njim     4\\n\")])])]),n(\"h2\",{attrs:{id:\"mapreduce\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce\"}},[t._v(\"#\")]),t._v(\" MapReduce\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"mapreduce-是什么？\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce-是什么？\"}},[t._v(\"#\")]),t._v(\" MapReduce 是什么？\")]),t._v(\" \"),n(\"blockquote\",[n(\"p\",[t._v('MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念\"Map（映射）\"和\"Reduce（归约）\"，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。它极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在'),n(\"a\",{attrs:{href:\"https://baike.baidu.com/item/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"分布式系统\"),n(\"OutboundLink\")],1),t._v(\"上。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。(摘自百度百科)\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"为什么要使用mr？\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#为什么要使用mr？\"}},[t._v(\"#\")]),t._v(\" 为什么要使用MR？\")]),t._v(\" \"),n(\"p\",[t._v(\"上面我们写的HDFS版wordcount程序：统计hdfs的/wordcount/input/a.txt文件中的单词出现个数\")]),t._v(\" \"),n(\"p\",[t._v(\"明白了一点：可以在任何地方运行程序，访问HDFS上的文件并进行统计运算，并且可以把统计结果写回HDFS的结果文件中。\")]),t._v(\" \"),n(\"p\",[t._v(\"但是，进一步思考：如果文件又多又大，用上面的代码有什么弊端？\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"慢！因为只有一台机器在进行运算处理。\")])]),t._v(\" \"),n(\"p\",[t._v(\"如何变的更快？\")]),t._v(\" \"),n(\"p\",[t._v(\"核心思路：让我们的程序并行在多台机器上执行。\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"mapreduce-框架\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce-框架\"}},[t._v(\"#\")]),t._v(\" MapReduce 框架\")]),t._v(\" \"),n(\"p\",[t._v(\"分两个阶段：\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"map阶段 -- 程序MapTask\")]),t._v(\" \"),n(\"li\",[t._v(\"reduce阶段 -- 程序ReduceTask\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"mapreduce具有的特点\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce具有的特点\"}},[t._v(\"#\")]),t._v(\" MapReduce具有的特点\")]),t._v(\" \"),n(\"p\",[t._v(\"总所周知MapReduce是一种很受欢迎的软件框架，尤其是我们国家发展到现在互联网的浪潮愈演愈烈，那么它都有什么特点呢？\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"strong\",[t._v(\"易于编程\")]),t._v(\"：MapReduce通过相应的接口，程序员只需要简单的调用就可以完成对一个复杂的分布式程序的编写。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"易扩展性\")]),t._v(\"：在计算资源不足时可以通过增加机器来增加计算能力。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"高容错性\")]),t._v(\"：要知道MapReduce的提出就是为了运行在廉价的商用pc中，而商用pc得到问题也是颇多，经常会出现pc挂掉的情况，这时候就需要可以迅速的把计算任何和资源转移到另外的一个节点上运行，从而保证任务、作业的顺利运行。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"海量PB级数据的离线处理\")]),t._v(\"：所谓离线处理即为它不具体毫秒级别的迅速反馈能力，在对反馈要求非常及时的场景下，自然是不可用的\")])]),t._v(\" \"),n(\"p\",[t._v(\"那么MapReduce有哪些不适合的场景呢？\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"strong\",[t._v(\"实时计算\")]),t._v(\"：没有mysql等数据库的毫秒级反馈能力。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"流式计算\")]),t._v(\"：流式计算的输入数据时动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。\")]),t._v(\" \"),n(\"li\",[n(\"strong\",[t._v(\"DAG(有向图)模式\")]),t._v(\"：即每个作业或者任务之间都有很强的连接性，下一个作业的运行需要另外一个作业的运行结果的数据，这种情况下MapReduce的性能非常低，因为每个MapReduce的作业都会把计算写入到磁盘中，若如此做则会造成大量的磁盘IO,性能低下。\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"mapreduce-实例\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce-实例\"}},[t._v(\"#\")]),t._v(\" MapReduce 实例\")]),t._v(\" \"),n(\"p\",[t._v(\"为了分析 MapReduce 的编程模型，这里我们以 WordCount 为实例。就像 Java、C++等编程语言的入门程序 hello word 一样，WordCount 是 MapReduce 最简单的入门程序。下面我们就来逐步分析。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"1. 场景\")]),t._v(\"：假如有大量的文件，里面存储的都是单词。\")]),t._v(\" \"),n(\"p\",[t._v(\"类似应用场景：WordCount 虽然很简单，但它是很多重要应用的模型。\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[t._v(\"搜索引擎中，统计最流行的 K 个搜索词。\")]),t._v(\" \"),n(\"li\",[t._v(\"统计搜索词频率，帮助优化搜索词提示。\")])]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"2. 任务\")]),t._v(\"：我们该如何统计每个单词出现的次数？\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"3. 将问题规范为\")]),t._v(\"：有一批文件（规模为 TB 级或者 PB 级），如何统计这些文件中所有单词出现的次数。\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"4. 解决方案\")]),t._v(\"：首先，分别统计每个文件中单词出现的次数；然后，累加不同文件中同一个单词出现次数。\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"mapreduce-版-wordcount-代码实现\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce-版-wordcount-代码实现\"}},[t._v(\"#\")]),t._v(\" MapReduce 版 wordcount 代码实现\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"第一步：导入maven依赖\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"\\x3c!-- hadoop-client --\\x3e\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"dependency\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"groupId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"org.apache.hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"groupId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"artifactId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hadoop-client\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"artifactId\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"version\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"2.10.0\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"version\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"dependency\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"第二步：编写WordcountMapper.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"package\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"com\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"pyy\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mr\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LongWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mapreduce\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Mapper\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"/**\\n * KEYIN: 是一行的起始偏移量 Long\\n * VALUEIN：是一行的内容 String\\n *\\n * KEYOUT：用户自定义map方法要返回的结果kv数据的key的类型，在wordcount中，我们需要返回的是单词 String\\n * VALUEOUT： 用户自定义map方法要返回的结果kv数据的value的类型，在wordcount中，我们需要返回的是整数 Integer\\n *\\n * 但是，MapReduce中，map产生的数据要传输为 reduce，需要进行序列化和反序列化，而原生jdk序列化产生的数据量比较冗余\\n * 就会导致数据在MapReduce运行过程效率低，所有，hadoop专门设计了自己的序列化机制，那么，mapreduce中传输的数据类型就必须\\n * 要实现hadoop自己的序列化接口\\n *\\n * hadoop为jdk中的常用数据类型 Long String Integer Float等数据类型封装了自己的实现Hadoop序列化接口类型：LongWritable，Text，IntWritable, FloatWritable\\n */\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"WordcountMapper\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"extends\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Mapper\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LongWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Override\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"protected\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"map\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"LongWritable\")]),t._v(\" key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),t._v(\" value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"InterruptedException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 切单词\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" line \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"toString\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" words \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" line\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"split\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\" \"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"for\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),t._v(\" word \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" words\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"write\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"word\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"编写 WordcountReducer.java\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"package\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"com\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"pyy\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mr\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mapreduce\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Reducer\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"util\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Iterator\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"WordcountReducer\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"extends\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Reducer\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token annotation punctuation\"}},[t._v(\"@Override\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"protected\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"reduce\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),t._v(\" key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Iterable\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" values\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Context\")]),t._v(\" context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"InterruptedException\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"int\")]),t._v(\" count \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Iterator\")]),n(\"span\",{pre:!0,attrs:{class:\"token generics\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\" iterator \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" values\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"iterator\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"while\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"iterator\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"hasNext\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),t._v(\" value \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" iterator\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"next\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n            count \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"+=\")]),t._v(\" value\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"get\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n        context\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"write\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"key\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"count\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"mapreduce-执行流程\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mapreduce-执行流程\"}},[t._v(\"#\")]),t._v(\" MapReduce 执行流程\")]),t._v(\" \"),n(\"p\",[t._v(\"通过上面的分析可知，它其实就是一个典型的 MapReduce 过程。下面我们通过示意图来分析 MapReduce 过程。\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(289),alt:\"\"}})]),t._v(\" \"),n(\"p\",[t._v(\"上图流程大致分为以下几步：\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[t._v(\"第一步：假设一个文件有三行英文单词作为 MapReduce 的input（输入），这里经过 Splitting 过程把文件分割为3块。分割后的3块数据就可以并行处理，每一块交给一个 maptask 处理。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"第二步：每个 maptask 中，以每个单词为key，以1作为词频数value，然后输出。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"第三步：每个 maptask 的输出要经过 shuffling（混洗），将相同的单词key放在同一个桶里面，然后交给 reducetask 处理。\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"第四步：reduce 接收到 shuffling 后的数据，会将相同单词进行合并，得到每个单词的词频数，最后将统计好的每个单词的词频数作为输出结果。\")])])]),t._v(\" \"),n(\"p\",[t._v(\"上述就是 MapReduce 的大致流程，前两步可以看做是 Map 阶段，后两步可以看成 Reduce阶段。\")]),t._v(\" \"),n(\"p\",[t._v(\"具体如何执行，我们还需要借助于 YARN。\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"yarn简介\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#yarn简介\"}},[t._v(\"#\")]),t._v(\" YARN简介\")]),t._v(\" \"),n(\"blockquote\",[n(\"p\",[t._v(\"Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。（摘自百度百科）\")])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"yarn架构\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#yarn架构\"}},[t._v(\"#\")]),t._v(\" YARN架构\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:\"https://hadoop.apache.org/docs/r2.9.1/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif\",alt:\"MapReduce NextGen Architecture\"}})]),t._v(\" \"),n(\"p\",[t._v(\"yarn是一个分布式程序的\"),n(\"strong\",[t._v(\"运行调度平台\")])]),t._v(\" \"),n(\"p\",[t._v(\"yarn中有\"),n(\"strong\",[t._v(\"两大核心角色\")]),t._v(\"：\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"1、Resource Manager\")])]),t._v(\" \"),n(\"p\",[t._v(\"接受用户提交的分布式计算程序，并为其划分资源\")]),t._v(\" \"),n(\"p\",[t._v(\"管理、监控各个Node Manager上的资源情况，以便于均衡负载\")]),t._v(\" \"),n(\"p\",[n(\"strong\",[t._v(\"2、Node Manager\")])]),t._v(\" \"),n(\"p\",[t._v(\"管理它所在机器的运算资源（cpu + 内存）\")]),t._v(\" \"),n(\"p\",[t._v(\"负责接受Resource Manager分配的任务，创建容器、回收资源\")]),t._v(\" \"),n(\"h3\",{attrs:{id:\"yarn-的安装\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#yarn-的安装\"}},[t._v(\"#\")]),t._v(\" YARN 的安装\")]),t._v(\" \"),n(\"p\",[t._v(\"NodeManager 在物理上应该跟 DataNode 部署在一起（方便任务执行）通过slaves文件设置\")]),t._v(\" \"),n(\"p\",[t._v(\"ResourceManager 最好单独部署在一台专门的机器上。\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[n(\"p\",[t._v(\"修改配置文件：yarn-site.xml\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"yarn.resourcemanager.hostname\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"hdp-01\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"yarn.nodemanager.aux-services\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"mapreduce_shuffle\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"mapreduce.framework.name\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"yarn\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"拷贝配置文件：mapred-site.xml.template mapred-site.xml 设置job提交到哪里运行\")]),t._v(\" \"),n(\"div\",{staticClass:\"language- extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[n(\"code\",[t._v(\"<configuration>\\n  <property>\\n  \\t<name>mapreduce.framework.name</name>\\n  \\t<value>yarn</value>\\n\\t</property>\\n</configuration>\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"scp 这个 yarn-site.xml mapped-site.xml 到其它节点\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"启动 yarn 集群：start-yarn.sh （注：该命令应该在 ResourceManager 所在的机器上执行,就会在该机器上启动 RM）\")])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"用 \"),n(\"code\",[t._v(\"jps\")]),t._v(\" 检查 yarn 的进程，用 web 浏览器查看 yarn 的 web 控制台。\")]),t._v(\" \"),n(\"p\",[t._v(\"http://10.10.50.189:8088\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(290),alt:\"\"}})])])]),t._v(\" \"),n(\"h3\",{attrs:{id:\"运行上面的-wordcount-mr程序\"}},[n(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#运行上面的-wordcount-mr程序\"}},[t._v(\"#\")]),t._v(\" 运行上面的 wordcount mr程序\")]),t._v(\" \"),n(\"ol\",[n(\"li\",[n(\"p\",[t._v(\"接着上面的 wordcount 工程，编写 JobSubmitter 客户端类\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"conf\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fs\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mapreduce\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Job\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mapreduce\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"lib\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"input\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"org\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"apache\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"hadoop\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mapreduce\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"lib\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"output\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileOutputFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token namespace\"}},[t._v(\"java\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"io\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IOException\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"/**\\n * 提交 mapreduce job 的客户端\\n * 功能：\\n *  1. 封装本次job运行时需要的必要参数\\n *  2. 跟yarn进行校核，将mapreduce程序成功的启动、运行\\n */\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"JobSubmitter\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"public\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"static\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"void\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"main\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"String\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),t._v(\" args\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"throws\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Exception\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setProperty\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"HADOOP_USER_NAME\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"root\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),t._v(\" conf \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        conf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"core-site.xml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        conf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"hdfs-site.xml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        conf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"yarn-site.xml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        conf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"addResource\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mapred-site.xml\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 如果要从windows上运行这个job提交客户端程序，则需要加这个跨平台提交的参数\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v('//conf.set(\"mapreduce.app-submission.cross-platform\", \"true\");')]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),t._v(\" fs \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileSystem\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"newInstance\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"conf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Job\")]),t._v(\" job \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Job\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"getInstance\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"conf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 1.封装参数：jar包所在位置\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v('// job.setJar(\"/wc.jar\")')]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setJarByClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"JobSubmitter\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 2.封装参数：本地job所要调用的Mapper实现类、Reducer实现类\")]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setMapperClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"WordcountMapper\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setReducerClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"WordcountReducer\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 3.封装参数：本次job的Mapper实现类、Reducer实现类产生的结果数据的key、value类型\")]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setMapOutputKeyClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setMapOutputValueClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setOutputKeyClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Text\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setOutputValueClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"IntWritable\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),t._v(\" output \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/wordcount/output/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"if\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"exists\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"output\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"{\")]),t._v(\"\\n            fs\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"delete\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"output\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 4.封装参数：本次job要处理的封装的输入数据集所在路径、最终结果的输出路径。\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileInputFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setInputPaths\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"new\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"Path\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/wordcount/input/\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"FileOutputFormat\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setOutputPath\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" output\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//注意：必须保证路径不存在\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 5.封装参数：想要启动的 reducer task的数量（默认1） （map task 自己根据切片自动）\")]),t._v(\"\\n        job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setNumReduceTasks\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"2\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 6.提交job给yarn\")]),t._v(\"\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"boolean\")]),t._v(\" res \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"waitForCompletion\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token boolean\"}},[t._v(\"true\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n        \"),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"System\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"exit\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"res \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"?\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\":\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"-\")]),n(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"1\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\\n    \"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"}\")]),t._v(\"\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"运行方式\")]),t._v(\" \"),n(\"ul\",[n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"远程jar包部署\")])]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 设置jar类加载器，否则找不到Mapper和Reducer\")]),t._v(\"\\njob\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setJarByClass\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token class-name\"}},[t._v(\"JobSubmitter\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"class\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\")])])]),n(\"div\",{staticClass:\"language-shell extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-shell\"}},[n(\"code\",[t._v(\"打包 mvn clean package\\n上传 jar 包到 linux 系统\\n保证 hdfs 和 \"),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"yarn\")]),t._v(\" 都正常启动和运行\\n\\n通过 hadoop jar 命令运行\\nhadoop jar wc-1.0-SNAPSHOT.jar com.pyy.mr.JobSubmitter\\n\")])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[n(\"strong\",[t._v(\"跨平台提交\")])]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"// 如果要从非linux上运行这个job提交客户端程序，则需要加这个跨平台提交的参数\")]),t._v(\"\\nconf\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"set\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"mapreduce.app-submission.cross-platform\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"true\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"或配置 mapred-site.xml\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-xml extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-xml\"}},[n(\"code\",[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"mapreduce.framework.name\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n  \\t\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"yarn\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"mapreduce.app-submission.cross-platform\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"name\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"<\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"true\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"value\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\\t\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"property\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\"),n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token tag\"}},[n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"</\")]),t._v(\"configuration\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\">\")])]),t._v(\"\\n\")])])]),n(\"p\",[t._v(\"设置jar包路径\")]),t._v(\" \"),n(\"div\",{staticClass:\"language-java extra-class\"},[n(\"pre\",{pre:!0,attrs:{class:\"language-java\"}},[n(\"code\",[t._v(\" job\"),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),n(\"span\",{pre:!0,attrs:{class:\"token function\"}},[t._v(\"setJar\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),n(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v('\"/wc.jar\"')]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),n(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\";\")]),n(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"//系统会通过网络将jar删除到yarn\")]),t._v(\"\\n\")])])])])])]),t._v(\" \"),n(\"li\",[n(\"p\",[t._v(\"去hdfs的输出目录查看结果。\")]),t._v(\" \"),n(\"p\",[n(\"img\",{attrs:{src:a(291),alt:\"\"}})])])])])}),[],!1,null,null,null);s.default=e.exports}}]);","extractedComments":[]}