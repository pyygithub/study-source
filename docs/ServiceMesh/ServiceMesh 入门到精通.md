# ServiceMesh 入门到精通

## 第一章 架构的发展历史

**发展历史时间轴**

![1588598088789](./img/1588598088789.png)

### 1.1 单机小型机时代

第一个计算机网络诞生于1969年，也就是美军的阿帕网，阿帕网能够实现与其它计算机进行联机操作，但是早期仅仅是为了军事目的而服务
2000年初，中国的网民大约890万，很多人都不知道互联网为何物，因此大多数服务业务单一且简单，采用典型的单机+数据库模式，所有的功能都写在一个应用里并进行集中部署

![1587454159260](./img/1587454159260.png)

 论坛业务、聊天室业务、邮箱业务全部都耦合在一台小型机上面，所有的业务数据也都存储在一台数据库上。


### 1.2 垂直拆分

随着应用的日益复杂与多样化，开发者对系统的容灾，伸缩以及业务响应能力有了更高的要求，如果小型机和数据库中任何一个出现故障，整个系统都会崩溃，若某个板块的功能需要更新，那么整个系统都需要重新发布，显然，对于业务迅速发展的万物互联网时代是不允许的。
如何保障可用性的同时快速响应业务的变化，需要将系统进行拆分，将上面的应用拆分出多个子应用。![1587454061860](./img/1587454061860.png)

 优点：应用跟应用解耦，系统容错提高了，也解决了独立应用发布的问题

 应用垂直拆分解决了应用发布的问题，但是随着用户数量的增加，单机的计算能力依旧是杯水车薪

### 1.3 集群化负载均衡架构


用户量越来越大，就意味着需要更多的小型机，但是小型机价格昂贵，操作维护成本高。
此时更优的选择是采用多台PC机部署同一个应用的方案，但是此时就需要对这些应用做负载均衡，因为客户端不知道请求会落到哪一个后端PC应用上的。



负载均衡可以分为硬件层面和软件层面。
硬件层面：F5
软件负载层面：LVS、Nginx、Haproxy
负载均衡的思想：对外暴露一个统一的接口，根据用户的请求进行对应规则转发，同时负载均衡还可以做限流等等


有了负载均衡之后，后端的应用可以根据流量的大小进行动态扩容，我们称为"水平扩展"


![1588312438637](./img/1588312438637.png)

 阿里巴巴在2008提出去“IOE”，也就是IBM小型机、Oracle数据库，EMC存储，全部改成集群化负载均衡架构，在2013年支付宝最后一台IBM小型机下线

 优点：应用跟应用解耦，系统容错提高了，也解决了独立应用发布的问题，同时可以水平扩展来提供应用的并发量

### 1.4 服务化改造架构

虽然系统经过了垂直拆分，但是拆分之后发现在论坛和聊天室中有重复的功能，比如，用户注册、发邮件等等，一旦项目大了，集群部署多了，这些重复的功能无疑会造成资源浪费，所以会把重复功能抽取出来，名字叫"XX服务（Service）"

![1588265625571](./img/1588265625571.png)

 为了解决服务跟服务如何相互调用，需要一个程序之间的通信协议，所以就有了远程过程调用（RPC），作用就是让服务之间的程序调用变得像本地调用一样的简单

 优点：在前面的架构之上解决了业务重用的问题

### 1.5 服务治理

随着业务的增大，基础服务越来越多，调用网的关系由最初的几个增加到几十上百，造成了调用链路错综复杂,需要对服务进行治理。

服务治理要求：

1. 当我们服务节点数几十上百的时候，需要对服务有动态的感知，引入了注册中心
2. 当服务链路调用很长的时候如何实现链路的监控
3. 单个服务的异常，如何能避免整条链路的异常（雪崩），需要考虑熔断、降级、限流
4. 服务高可用：负载均衡

典型框架比如有：Dubbo,默认采用的是Zookeeper作为注册中心。


### 1.6 微服务时代

 **分布式微服务时代**

微服务是在2012年提出的概念，微服务的希望的重点是一个服务只负责一个独立的功能。

拆分原则，任何一个需求不会因为发布或者维护而影响到不相关的服务，一切可以做到独立部署运维。

比如传统的“用户中心”服务，对于微服务来说，需要根据业务再次拆分，可能需要拆分成“买家服务”、“卖家服务”、“商家服务”等。

典型代表：Spring Cloud，相对于传统分布式架构，SpringCloud使用的是HTTP作为RPC远程调用，配合上注册中心Eureka和API网关Zuul，可以做到细分内部服务的同时又可以对外暴露统一的接口，让外部对系统内部架构无感，此外Spring Cloud的config组件还可以把配置统一管理。

 马丁大师对微服务的定义：https://martinfowler.com/articles/microservices.html

 微服务真正定义的时间是在2014年


 The term "Microservice Architecture" has sprung up over the last few years to describe a particular way of designing software applications as suites of independently deployable services. While there is no precise definition of this architectural style, there are certain common characteristics around organization around business capability, automated deployment, intelligence in the endpoints, and decentralized control of languages and data.


  大概意思：可独立部署服务，服务会越来越细

> spring cloud地址：https://spring.io/projects/spring-cloud

### 1.7 服务网格新时期

**早期**

我们最开始用Spring+SpringMVC+Mybatis写业务代码

**微服务时代**

微服务时代有了Spring Cloud就完美了吗？不妨想一想会有哪些问题?

- 最初是为了业务而写代码，比如登录功能、支付功能等，到后面会发现要解决网络通信的问题，虽然有 Spring Cloud里面的组件帮我们解决了，但是细想一下它是怎么解决的？在业务代码里面加上spring cloud maven依赖，加上spring cloud组件注解，写配置，打成jar的时候还必须要把非业务的代码也要融合在一起，称为“侵入式框架”； 

- 微服务中的服务支持不同语言开发，也需要维护不同语言和非业务代码的成本； 

- 业务代码开发者应该把更多的精力投入到业务熟悉度上，而不应该是非业务上，Spring Cloud虽然能解决微服务领域的很多问题，但是学习成本还是较大的；  

- 互联网公司产品的版本升级是非常频繁的，为了维护各个版本的兼容性、权限、流量等，因为SpringCloud是“代码侵入式的框架”，这时候版本的升级就注定要让非业务代码一起，一旦出现问题，再加上多语言之间的调用，工程师会非常痛苦； 

- 其实我们到目前为止应该感觉到了，服务拆分的越细，只是感觉上轻量级解耦了，但是维护成本却越高了，那么怎么 办呢？

 我们不是说spring cloud不好，只是为了引出service mesh, 目前spring cloud微服务还是比较主流的， 我们指出spring cloud的不好也只是为了突出service mesh的优点

**问题解决思路**

- 本质上是要解决服务之间通信的问题，不应该将非业务的代码融合到业务代码中 
- 也就是从客户端发出的请求，要能够顺利到达对应的服务，这中间的网络通信的过程要和业务代码尽量无关
- 服务通信无非就是服务发现、负载均衡、版本控制等等
- 在很早之前的单体架构中，其实通信问题也是需要写在业务代码中的，那时候怎么解决的呢？

![1587462497556](./img/1587462497556.png)

**解决方案：**把网络通信，流量转发等问题放到了计算机网络模型中的TCP/UDP层，也就是非业务功能代码下沉，把这些网络的问题下沉到计算机网络模型当中，也就是网络七层模型
网络七层模型：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层


![1587462783887](./img/1587462783887.png)

  **思考：**

  我们是否也可以把每个服务配置一个代理，所有通信的问题都交给这个代理去做，就好比大家熟悉的nginx,haproxy其实它们做反向代理把请求转发给其他服务器,也就为 Service Mesh的诞生和功能实现提供了一个解决思路 

#### SideCar

它降低了与微服务架构相关的复杂性，并且提供了负载平衡、服务发现、流量管理、电路中断、遥测、故障注入等功能特性。
Sidecar模式是一种将应用功能从应用本身剥离出来作为单独进程的方式。该模式允许我们向应用无侵入添加多种功能，避免了为满足第三方组件需求而向应用添加额外的配置代码。

很多公司借鉴了Proxy模式，推出了Sidecar的产品，比如像Netflix的Prana，蚂蚁金服的SofaMesh

![1587533226466](./img/1587533226466.png)


服务业务代码与Sidecar绑定在一起，每个服务都配置了一个Sidecar代理，每个服务所有的流量都经过sidecar,sidecar帮我们屏蔽了通信的细节，我的业务开发人员只需要关注业务就行了，而通信的事情交给sidecar处理

> 总结：可以理解成是代理，控制了服务的流量的进出，sidecar是为了通用基础设施而设计，可以做到与公司框架技术无侵入性

**SideCar的探索之路还在继续**

很多公司借鉴了Proxy模式，推出了Sidecar的产品，比如像Netflix的Prana，蚂蚁金服的SofaMesh

- 2014年 Netflix发布的Prana
- 2015年 唯品会发布local proxy
- 2016年 Twitter的基础设施工程师发布了第一款Service Mesh项目：Linkerd (所以下面介绍Linkerd)

#### Linkerd

2016年1月，离开Twitter的基础设施工程师打造的一个服务网格项目,第一个Service Mesh项目由此诞生，解决通用性。
Linkerd很好地结合了Kubernetes所提供的功能，以此为基础，在每个Kubernetes Node上都部署运行一个Linkerd实例，用代理的方式将加入Mesh的Pod通信转接给Linkerd，这样Linkerd就能在通信链路中完成对通信的控制和监控。


**Linkerd设计思想**

![1587533995800](./img/1587533995800.png)

Linderd的思想跟sidecar很类似，目标也是屏蔽网络通信细节

Linkerd除了完成对Service Mesh的命名，以及Service Mesh各主要功能的落地，还有以下重要创举：

* 无须侵入工作负载的代码，直接进行通信监视和管理；
* 提供了统一的配置方式，用于管理服务之间的通信和边缘通信；
*  除了支持Kubernetes，还支持多种底层平台。

**总结：**

* 跟我们前面sidecar很类似，以前的调用方式都是服务来调用服务，在Linkerd思想要求所有的流量都走sidecar，Linkerd帮业务人员屏蔽了通信细节，通信不需要侵入到业务代码内部了，这样业务开发者就专注于业务开发的本身
* Linkerd在面世之后，迅速获得用户的关注，并在多个用户的生产环境上成功部署、运行。2017年，Linkerd加入CNCF，随后宣布完成对千亿次生产环境请求的处理，紧接着发布了1.0版本，并且具备一定数量的商业用户，一时间风光无限，一直持续到Istio横空出世。

**问题:** 

在早期的时候又要部署服务，又要部署sidecar，对于运维人员来说比较困难的，所以没有得到很好的发展，其实主要的问题是Linkerd只是实现了数据层面的问题，但没有对其进行很好的管理。 

> 数据层面: 通过sidecar解决了数据处理的问题
>
> 打开github: 搜索linkerd，是由scala语言编写的


#### istio

由Google、IBM和Lyft共同发起的开源项目

打开github:搜索istio，是由go语言编写的

**什么是istio?**

> `地址`：https://istio.io/docs/concepts/what-is-istio/#why-use-istio

> Istio makes it easy to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, with few or no code changes in service code. You add Istio support to services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, then configure and manage Istio using its control plane functionality
> 翻译：
> 通过Istio，可以轻松创建带有负载平衡，服务到服务的身份验证，监视等功能的已部署服务网络，使得服务中的代码更改很少或没有更改。 通过在整个环境中部署一个特殊的sidecar代理来拦截微服务之间的所有网络通信，然后使用其控制平面功能配置和管理，可以为服务添加Istio支持。

> **注意这句话：**
>
> **使得服务中的代码更改很少或没有更改**
>
> 这句描述非常重要，如果我们用的是spring cloud通信功能，是不是要加依赖、加注解、改配置

**什么是控制平面？**

控制平面就是来管理数据平面，也就是管理sideCar
 ![1588330903686](./img/1588330903686.png)

**所以istio既有数据平面也有控制平面**

**istio能干什么?**

> Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.
> Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault injection.
> A pluggable policy layer and configuration API supporting access controls, rate limits and quotas.
> Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and egress.
> Secure service-to-service communication in a cluster with strong identity-based authentication and authorization.
>
> 翻译：
> 1.HTTP、gRPC、WebSocket和TCP流量的自动负载平衡。
> 2.路由、重试、故障转移和故障注入对流量行为进行细粒度控制。
> 3.支持访问控制、速率限制、配置API。
> 4.集群内所有流量的自动衡量、日志和跟踪，包括集群入口和出口。
> 5.使用基于身份验证和授权来保护集群中服务跟服务之间的通信。

**总结：**很明显Istio不仅拥有“数据平面（Data Plane）”，而且还拥有“控制平面（Control Plane），也就是拥有了数据 接管与集中控制能力。

#### 什么是服务网格

服务网格：指的是微服务网络应用之间的交互，随着规模和复杂性增加，服务跟服务调用错综复杂



![1589347876849](./img/1589347876849.png)

如果每一个格子都是一个sidecar数据平面，然后sidecar进行彼此通信，那么servicemech就是来管理每个格子的控制平面,这就是服务网格，从架构层面看起来跟网格很像。

**特点：**

- 基础设施：服务网格是一种处理服务之间通信的基础设施层。
- 支撑云原生：服务网格尤其适用于在云原生场景下帮助应用程序在复杂的服务间可靠地传递请求。
- 网络代理：在实际使用中，服务网格一般是通过一组轻量级网络代理来执行治理逻辑的。
- 对应用透明：轻量网络代理与应用程序部署在一起，但应用感知不到代理的存在，还是使用原来的方式工作。

#### 什么是Service Mesh

 **istio官网 也对什么是service mesh给出了定义**

> `地址`：https://istio.io/docs/concepts/what-is-istio/#what-is-a-service-mesh

> Istio addresses the challenges developers and operators face as monolithic applications transition towards a distributed microservice architecture. To see how, it helps to take a more detailed look at Istio’s service mesh.
>
> 翻译：
>  解决开发与运维部署分布式微服务面临的问题



> The term service mesh is used to describe the network of microservices that make up such applications and the interactions between them. As a service mesh grows in size and complexity, it can become harder to understand and manage. Its requirements can include discovery, load balancing, failure recovery, metrics, and monitoring. A service mesh also often has more complex operational requirements, like A/B testing, canary rollouts, rate limiting, access control, and end-to-end authentication.
>
> 翻译：
> 也是解决微服务之间服务跟服务之间通信的问题，可以包括服务发现、负载平衡、故障恢复、度量和监视，服务网格通常还具有更复杂的操作需求，如A/B测试、速率限制、访问控制和端到端身份验证

#### CNCF云原生组织发展和介绍

**云原生发展历史时间轴**

![1589352593807](./img/1589352593807.png)

- **微服务**

  马丁大师在2014年定义了微服务 

- **Kubernetes**

  从2014年6月由Google宣布开源，到2015年7月发布1.0这个正式版本并进入CNCF基金会，再到2018年3月从CNCF基金会正式毕业，迅速成为容器编排领域的标准，是开源历史上发展最快的项目之一

- **Linkerd**

  Scala语言编写，运行在JVM中，Service Mesh名词的创造者 
  2016年01月15号，0.0.7发布 
  2017年01月23号，加入CNCF组织 
  2017年04月25号，1.0版本发布


- **Envoy**

  envoy是一个开源的服务代理，为云原生设计的程序，由C++语言编程[Lyft] 
  2016年09月13号，1.0发布 
  2017年09月14号，加入CNCF组织

- **Istio**

  Google、IBM、Lyft发布0.1版本
  Istio是开源的微服务管理、保护和监控框架。Istio为希腊语，意思是”起航“。

**CNCF介绍**

CNCF 是一个开源软件基金会，致力于使云原生计算具有普遍性和可持续性。 云原生计算使用开源软件技术栈将应用程序部署为微服务，将每个部分打包到自己的容器中，并动态编排这些容器以优化资源利用率。 云原生技术使软件开发人员能够更快地构建出色的产品。

**CNCF解决了什么问题**

  * 统一基础平台：kubernetes

  * 如果我们需要日志监控：Prometheus

  * 需要代理：Envoy
  * 需要分布式链路跟踪：Jaeger
  * ......

> 地址`：https://www.cncf.io/

**介绍几个常用的已经毕业的云原生项目**

   * Kubernetes

     Kubernetes 是世界上最受欢迎的容器编排平台也是第一个 CNCF 项目。 Kubernetes 帮助用户构建、扩展和管理应用程序及其动态生命周期。


   * Prometheus

     Prometheus 为云原生应用程序提供实时监控、警报包括强大的查询和可视化能力，并与许多流行的开源数据导入、导出工具集成。


   * Jaeger

     Jaeger 是由 Uber 开发的分布式追踪系统，用于监控其大型微服务环境。 Jaeger 被设计为具有高度可扩展性和可用性，它具有现代 UI，旨在与云原生系统（如 OpenTracing、Kubernetes 和 Prometheus）集成。


   * Containerd

     Containerd 是由 Docker 开发并基于 Docker Engine 运行时的行业标准容器运行时组件。 作为容器生态系统的选择，Containerd 通过提供运行时，可以将 Docker 和 OCI 容器镜像作为新平台或产品的一部分进行管理。


   * Envoy

     Envoy 是最初在 Lyft 创建的 Service Mesh（服务网格），现在用于Google、Apple、Netflix等公司内部。 Envoy 是用 C++ 编写的，旨在最大限度地减少内存和 CPU 占用空间，同时提供诸如负载均衡、网络深度可观察性、微服务环境中的跟踪和数据库活动等功能。


   * Fluentd

     Fluentd 是一个统一的日志记录工具，可收集来自任何数据源（包括数据库、应用程序服务器、最终用户设备）的数据，并与众多警报、分析和存储工具配合使用。 Fluentd 通过提供一个统一的层来帮助用户更好地了解他们的环境中发生的事情，以便收集、过滤日志数据并将其路由到许多流行的源和目的地。  

  **孵化中的项目**

   * Open Tracing

     OpenTracing：为不同的平台，供应中立的API，使开发人员可以轻松地应用分布式跟踪。


   * GRPC

     gRPC 是一个高性能、开源和通用的 RPC 框架,语言中立，支持多种语言。


   * CNI

     CNI 就是这样一个标准，它旨在为容器平台提供网络的标准化。不同的容器平台能够通过相同的接口调用不同的网络组件。


   * Helm

     Helm 是 Kubernetes 的包管理器。包管理器类似于我们在Centos中使用的yum一样，能快速查找、下载和安装软件包。


   * Etcd

     一个高可用的分布式键值(key-value)数据库。etcd内部采用raft协议作为一致性算法，etcd基于Go语言实现。一般用的最多的就是作为一个注册中心来使用

     

#### 国内兴起的服务网格


前面提到，在Service Mesh这个概念得到具体定义之前，实际上已经有很多厂商开始了微服务新的尝试，这一动作势必引发对微服务治理的强劲需求。在Service Mesh概念普及之后，有的厂商意识到自身产品也具备Service Mesh的特点，也有厂商受其启发，将自有的服务治理平台进行完善和改造，推出自己的Service Mesh产品。例如，蚂蚁金服、腾讯和华为都推出自己的网格产品，华为的产品甚至已被投入公有云进行商业应用。

* 蚂蚁金服 sofa Mesh

   **代理架构**

   ![1588412147585](./img/1588412147585.png)

  前身是SOFA RPC ，2018年07月正式开源 

 * 腾讯 Tencent Service Mesh

   **代理架构**

    ![1588418151202](./img/1588418151202.png)

 * 华为  CSE Mesher

    **代理架构**

    ![1588418649120](./img/1588418649120.png)

    `总结:` 基本上都借鉴了Sidecar、Envoy和Istio等设计思想

## 第二章 Istio 基本介绍

### 2.1 什么是Istio 

> `地址`：https://istio.io/


服务网格是一个独立的基础设施层，用来处理服务之间的通信。现代的云原生应用是由各种复杂技术构建的服务体系，服务网格负责在这些组成部分之间进行可靠的请求传递。目前典型的服务网格通常提供了一组轻量级的网络代理，这些代理会在应用无感知的情况下，同应用并行部署、运行。

前面提到，Istio出自名门，由Google、IBM和Lyft在2017年5月合作推出，它的初始设计目标是在Kubernetes的基础上，以非侵入的方式为运行在集群中的微服务提供流量管理、安全加固、服务监控和策略管理等功能。

Istio有助于降低部署的复杂性，并减轻开发团队的压力。它是一个完全开放源代码的服务网格，透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志平台、遥测或策略系统中的api。Istio的多种功能集使我们能够成功、高效地运行分布式微服务体系结构，并提供一种统一的方式来保护、连接和监视微服务。


**传统的spring cloud微服务项目**

![1588420918416](./img/1588420918416.png)

**基于Istio架构微服务项目 **

![1588428082543](./img/1588428082543.png)

 Istio是基于Sidecar模式、数据平面和控制平面，是主流Service Mesh解决方案。

### 2.2 Istio特征

> `地址`：https://istio.io/zh/

* **连接**：对网格内部的服务之间的调用所产生的流量进行智能管理，并以此为基础，为微服务部署、测试和升级等操作提供有力保障。
*  **安全**：为网格内部的服务之间的调用提供认证、加密和鉴权支持，在不侵入代码的情况下，加固现有服务，提高其安全性。
* **策略**：在控制平面定制策略，并在服务中实施。

* **观察**：对服务之间的调用进行跟踪和测量，获取服务的状态信息。

#### 连接

微服务错综复杂，要完成其业务目标，连接问题是首要问题。连接存在于所有服务的整个生命周期中，用于维持服务的运行，算得上重中之重。

相对于传统的单体应用，微服务的端点数量会急剧增加，现代的应用系统在部分或者全部生命周期中，都存在同一服务的不同版本，为不同的客户、场景或者业务提供不同的服务。同时，同一服务的不同版本也可能有不同的访问要求，甚至产生了在生产环境中进行测试的新方法论。错综复杂的服务关系对开发者来说都是很严峻的考验。


针对目前的常见业务形态，这里画一个简单的示意图来描述Service Mesh的连接功能

 ![1587653293503](./img/1587653293503.png)

从不同的外部用户的角度来看，他们访问的都是同一服务端口，但实际上会因为不同的用户识别，分别访问服务A的不同版本；在网格内部，服务A的版本1可能会访问服务B的两个版本，服务A的版本2则只会访问服务B的版本1；服务B的版本1需要访问外部的云服务，版本2则无此需求。

> **在这个简化的模型中，包含了以下诉求：**

- 网格内部的调用（服务A→服务B）

- 出站连接（服务B→外部云服务）

- 入站连接（用户→服务A）

- 流量分割（A服务跟B服务只负责与自己相关流量请求）

- 按调用方的服务版本进行路由（服务A的版本1分别调用了服务B的版本1和版本2）

- 按用户身份进行路由。

  

> **这里除了这些问题，还存在一些潜在需求，如下所述：**

- 在网格内部的服务之间如何根据实际需要对服务间调用进行路由，条件可能包括：
  - 调用的源和目的服务
  - 调用内容
  - 认证身份

- 如何应对网络故障或者服务故障。

- 如何处理不同服务不同版本之间的关系。

- 怎样对出站连接进行控制。

- 怎样接收入站连接来启动后续的整个服务链条。

  

> **这些当然不是问题的全部，其中，与流量相关的问题还引发了几个关键的功能需求，如下所述：**

- 服务注册和发现：要求能够对网格中不同的服务和不同的版本进行准确标识，不同的服务可以经由同一注册机构使用公认的方式互相查找。
- 负载均衡策略：不同类型的服务应该由不同的策略来满足不同的需要。
- 服务流量特征：在服务注册发现的基础之上，根据调用双方的服务身份，以及服务流量特征来对调用过程进行甄别。
- 动态流量分配：根据对流量特征的识别，在不同的服务和版本之间对流量进行引导。

 连接是服务网格应用过程中从无到有的最重要的一个环节。

#### 安全

安全也是一个常谈常新的话题，在过去私有基础设施结合单体应用的环境下，这一问题并不突出，然而进入容器云时代之后，以下问题出现了:

- 有大量容器漂浮在容器云中，采用传统的网络策略应对这种浮动的应用是比较吃力的。
- 在由不同的语言、平台所实现的微服务之间，实施一致的访问控制也经常会因为实现的不一致而困难重重。
- 如果是共享集群，则服务的认证和加密变得尤为重要，例如：
  - 服务之间的通信要防止被其他服务监听；
  - 只有提供有效身份的客户端才可以访问指定的服务；
  -  服务之间的相互访问应该提供更细粒度的控制功能。

总之，要提供网格内部的安全保障，就应具备服务通信加密、服务身份认证和服务访问控制（授权和鉴权）功能。
上述功能通常需要数字证书的支持，这就隐藏了对CA的需求，即需要完成证书的签发、传播和更新业务。
除了上述核心要求，还存在对认证失败的处理、外部证书（统一 CA）的接入等相关支撑内容。

#### 策略

![1589360817797](./img/1589360817797.png)




Istio 通过可动态插拔、可扩展的策略实现访问控制、速率限制、配额管理等功能使得资源在消费者之间公平分配

在Istio中使用Mixer作为策略的执行者，Envoy的每次调用，在逻辑上都会通过Mixer进行事先预检和事后报告，这样Mixer就拥有了对流量的部分控制能力；在Istio中还有为数众多的内部适配器及进程外适配器，可以和外部软件设施一起完成策略的制定和执行。

- Mixer: Mixer 在整个服务网格中执行访问控制和策略执行，并从 Envoy 代理和其他服务收集遥测数据。

- Envoy: 在istio框架中使用Envoy作为代理，使用的是C++开发的软件，用于为服务网格中的所有服务提供所有的入站和出站流量,唯一一个与数据平面打交道的

#### 观察

随着服务数量的增加，监控和跟踪需求自然水涨船高。在很多情况下，可观察的保障都是系统功能的重要组成部分，是系统运维功能的重要保障。

随着廉价服务器（相对于传统小型机）的数量越来越多，服务器发生故障的频率也越来越高，人们开始产生争论：我们应该将服务器视为家畜还是宠物？家畜的特点：是有功能、无个性、可替换；而宠物的特点：是有功能、有个性、难替换。

我们越来越倾向于将服务器视为无个性、可替换的基础设施，如果主机发生故障，那么直接将其替换即可；并且，我们更加关注的是服务的总体质量。因此，微服务系统监控，除了有传统的主机监控，还更加重视高层次的服务健康监控。

服务的健康情况往往不是非黑即白的离散值，而是一系列连续状态，例如我们经常需要关注服务的调用成功率、响应时间、调用量、传输量等表现。

而且，面对数量众多的服务，我们应该能对各种级别和层次的指标进行采样、采集及汇总，获取较为精密、翔实的运行数据，最终通过一定的方法进行归纳总结和展示。

与此同时，服务网格还应提供分布式跟踪功能，对服务的调用链路进行跟踪。

观察性：动态获取服务运行数据和输出，提供强大的调用链、监控和调用日志收集输出的能力。配合可视化工具，可方便运维人员了解服务的运行状况，发现并解决问题。

### 2.3 Istio与服务治理


Istio是一个服务治理平台，治理的是服务间的访问，只要有访问就可以治理，不在乎这个服务是不是是所谓的微服务，也不要求跑的代码是微服务化的。单体应用不满足微服务用Istio治理也是完全可以的。提起“服务治理”，大家最先想到的一定是“微服务的服务治理”，就让我们从微服务的服务治理说起。

#### 服务治理的三种形态

服务治理的演变至少经过了以下三种形态。

**第1种形态：在应用程序中包含治理逻辑**

在微服务化的过程中，将服务拆分后会发现一堆麻烦事儿，连基本的业务连通都成了问题。在处理一些治理逻辑，比如怎么找到对端的服务实例，怎么选择一个对端实例发出请求，都需要自己写代码来实现。这种方式简单，对外部依赖少，但会导致存在大量的重复代码。

所以，微服务越多，重复的代码越多，维护越难；而且，业务代码和治理逻辑耦合，不管是对治理逻辑的全局升级，还是对业务的升级，都要改同一段代码。

![1588471998440](./img/1588471998440.png)

**第2种形态：治理逻辑独立的代码**

在解决第1种形态的问题时，我们很容易想到把治理的公共逻辑抽象成一个公共库，让所有微服务都使用这个公共库。在将这些治理能力包含在开发框架中后，只要是用这种开发框架开发的代码，就包含这种能力，非常典型的这种服务治理框架就是Spring Cloud。这种形态的治理工具在过去一段时间里得到了非常广泛的应用。

SDK模式虽然在代码上解耦了业务和治理逻辑，但业务代码和 SDK还是要一起编译的，业务代码和治理逻辑还在一个进程内。这就导致几个问题：业务代码必须和 SDK 基于同一种语言，即语言绑定。例如，Spring Cloud等大部分治理框架都基于Java，因此也只适用于 Java 语言开发的服务。经常有客户抱怨自己基于其他语言编写的服务没有对应的治理框架；在治理逻辑升级时，还需要用户的整个服务升级，即使业务逻辑没有改变，这对用户来说是非常不方便的。

![1588472030398](./img/1588472030398.png)

**第3种形态：治理逻辑独立的进程**


SDK模式仍旧侵入了用户的代码，那就再解耦一层，把治理逻辑彻底从用户的业务代码中剥离出来，这就是前面提过的Sidecar模式。显然，在这种形态下，用户的业务代码和治理逻辑都以独立的进程存在，两者的代码和运行都无耦合，这样可以做到与开发语言无关，升级也相互独立。在对已存在的系统进行微服务治理时，只需搭配 Sidecar 即可，对原服务无须做任何修改，并且可以对老系统渐进式升级改造，先对部分服务进行微服务化。

![1588472077426](./img/1588472077426.png)

> **总结**
>
> 比较以上三种服务治理形态，我们可以看到服务治理组件的位置在持续下沉，对应用的侵入逐渐减少。
>
> 微服务作为一种架构风格，更是一种敏捷的软件工程实践，说到底是一套方法论；与之对应的 Istio 等服务网格则是一种完整的实践，Istio 更是一款设计良好的具有较好集成及可扩展能力的可落地的服务治理工具和平台。
>
> 所以，微服务是一套理论，Istio是一种实践。

### 2.4 Istio与Kubernetes

#### Kubernetes介绍

Kubernetes是一款用于管理容器化工作的负载和服务的可移植、可扩展的开源平台，拥有庞大、快速发展的生态系统，它面向基础设施，将计算、网络、存储等资源进行紧密整合，为容器提供最佳运行环境，并面向应用提供封装好的、易用的工作负载与服务编排接口，以及运维所需的资源规格、弹性、运行参数、调度等配置管理接口，是新一代的云原生基础设施平台。

从平台架构而言，Kubernetes的设计围绕平台化理念，强调插件化设计与易扩展性，这是它与其他同类系统的最大区别之一，保障了对各种不同客户应用场景的普遍适应性。另外，Kubernetes与其他容器编排系统的显著区别是Kubernetes并不把无状态化、微服务化等条件作为可运行的工作负载的约束。

如今，容器技术已经进入产业落地期，而Kubernetes作为容器平台的标准已经得到了广泛应用。


![1588139405276](./img/1588139405276.png)

#### Istio是Kubernetes的好帮手

从场景来看，Kubernetes已经提供了非常强大的应用负载的部署、升级、扩容等运行管理能力。Kubernetes 中的 Service 机制也已经可以做服务注册、服务发现和负载均衡，支持通过服务名访问到服务实例。

从微服务的工具集观点来看，Kubernetes本身是支持微服务的架构，在Pod中部署微服务很合适，也已经解决了微服务的互访互通问题，但对服务间访问的管理如服务的熔断、限流、动态路由、调用链追踪等都不在Kubernetes的能力范围内。那么，如何提供一套从底层的负载部署运行到上层的服务访问治理端到端的解决方案？

目前，最完美的答案就是在Kubernetes上叠加Istio这个好帮手。


![1588477669403](./img/1588477669403.png)

#### Kubernetes是Istio的好基座

Istio最大化地利用了Kubernetes这个基础设施，与之叠加在一起形成了一个更强大的用于进行服务运行和治理的基础设施，充分利用了Kubernetes的优点实现Istio的功能，例如：

1. **数据面**

   数据面Sidecar运行在Kubernetes的Pod里，作为一个Proxy和业务容器部署在一起。在服务网格的定义中要求应用程序在运行的时感知不到Sidecar的存在。而基于Kubernetes的一个 Pod 多个容器的优秀设计使得部署运维
   对用户透明，用户甚至感知不到部署 Sidecar的过程。用户还是用原有的方式创建负载，通过 Istio 的自动注入服务，可以自动给指定的负载注入Proxy。如果在另一种环境下部署和使用Proxy，则不会有这样的便利。


![1588477896348](./img/1588477896348.png)

2. **统一服务发现**
   Istio的服务发现机制非常完美地基于Kubernetes的域名访问机制构建而成，省去了再搭一个类似 Eureka 的注册中心的麻烦，更避免了在 Kubernetes 上运行时服务发现数据不一致的问题。

![1588478040105](./img/1588478040105.png)



3. **基于Kubernetes CRD描述规则**
   Istio的所有路由规则和控制策略都是通过 Kubernetes CRD实现的，因此各种规则策略对应的数据也被存储在 Kube-apiserver 中，不需要另外一个单独的 APIServer 和后端的配置管理。所以，可以说Istio的APIServer就是Kubernetes的APIServer，数据也自然地被存在了对应Kubernetes的etcd中。

   Istio非常巧妙地应用了Kubernetes这个好基座，基于Kubernetes的已有能力来构建自身功能。Kubernetes里已经有的，绝不再自己搞一套，避免了数据不一致和用户使用体验的问题。

   Istio和Kubernetes架构的关系，可以看出，Istio不仅数据面Envoy跑在Kubernetes的Pod里，其控制面也运行在Kubernetes集群中，其控制面组件本身存在的形式也是以Kubernetes Deployment和Service，基于Kubernetes扩展和构建。



 **回顾一下上面提到的K8S组件**

 * APIServer

   API Server提供了k8s各类资源对象（pod,RC,Service等）增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。

   kubernetes API Server的功能：

   - 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)；
   - 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）;
   - 是资源配额控制的入口；
   - 拥有完备的集群安全机制.


 * Deployment

    一旦运行了 Kubernetes 集群，就可以在其上部署容器化应用程序。 为此，需要创建 Kubernetes Deployment 配置。
    Deployment 负责 Kubernetes 如何创建和更新应用程序的实例。


 * Service

   Service可以看作是一组提供相同服务的Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡


 * Ingress

   ingress是Kubernetes资源的一种，可以让外部请求访问到k8s内部的资源上

> **总结**
>
> Kubernetes在容器编排领域已经成为无可争辩的事实标准；微服务化的服务与容器在轻量、敏捷、快速部署运维等特征上匹配，这类服务在容器中的运行也正日益流行；随着Istio 的成熟和服务网格技术的流行，使用 Istio 进行服务治理的实践也越来越多，正成为服务治理的趋势；而 Istio 与 Kubernetes 的天然融合且基于 Kubernetes 构建，也补齐了Kubernetes的治理能力，提供了端到端的服务运行治理平台。
>
> 这都使得Istio、微服务、容器及Kubernetes形成一个完美的闭环。
>
> 云原生应用采用 Kubernetes 构建应用编排能力，采用 Istio 构建服务治理能力，将逐渐成为企业技术转型的标准配置。

### 2.5 Istio与服务网格

#### 时代选择服务网格

在云原生时代，随着采用各种语言开发的服务剧增，应用间的访问拓扑更加复杂，治理需求也越来越多。原来的那种嵌入在应用中的治理功能无论是从形态、动态性还是可扩展性来说都不能满足需求，迫切需要一种具备云原生动态、弹性特点的应用治理基础设施。

![1588498675957](./img/1588498675957.png)

采用Sidecar代理与应用进程的解耦带来的是应用完全无侵入、也屏蔽了开发语言无关等特点解除了开发语言的约束，从而极大降低了应用开发者的开发成本。

这种方式也经常被称为一种应用的基础设施层，类比TCP/IP网络协议栈，应用程序像使用TCP/IP一样使用这个通用代理：TCP/IP 负责将字节码可靠地在网络节点之间传递，Sidecar 则负责将请求可靠地在服务间进行传递。TCP/IP 面向的是底层的数据流，Sidecar 则可以支持多种高级协议（HTTP、gRPC、HTTPS 等），以及对服务运行时进行高级控制，使服务变得可监控、可管理。

然后，从全局来看，在多个服务间有复杂的互相访问时才有服务治理的需求。即我们关注的是这些 Sidecar 组成的网格，对网格内的服务间访问进行管理，应用还是按照本来的方式进行互相访问，每个应用程序的入口流量和出口流量都要经过Sidecar代理，并在Sidecar上执行治理动作。

最后，Sidecar是网格动作的执行体，全局的管理规则和网格内的元数据维护需要通过一个统一的控制面实现。
Sidecar拦截入口流量，执行治理动作。这就引入两个问题：

- 增加了两处延迟和可能的故障点；
-  多出来的这两跳对于访问性能、整体可靠性及整个系统的复杂度都带来了新的挑战。

![1588498713156](./img/1588498713156.png)

所以，对于考虑使用服务网格的用户来说，事情就会变成一个更简单的选择题：是否愿意花费额外的资源在这些基础设施上来换取开发、运维的灵活性、业务的非侵入性和扩展性等便利。

目前，华为、谷歌、亚马逊等云服务厂商将这种服务以云服务形态提供了出来，并和底层的基础设施相结合，提供了完整的服务治理解决方案。这对于广大应用开发者来说，更加方便和友好。

#### 服务网格选择Istio

在多种服务网格项目和产品中，最引人注目的是后来居上的 Istio，它有希望成为继Kubernetes之后的又一款重量级产品。

Istio 解决了生产大规模集群的性能、资源利用率和可靠性问题，提供了众多生产中实际应用的新特性，已经达到企业级可用的标准。

首先，在控制面上，Istio作为一种全新的设计，在功能、形态、架构和扩展性上提供了远超服务网格的能力范围。它提供了一套标准的控制面规范，向数据面传递服务信息和治理规则。

Istio使用Envoy V2版本的API，即gRPC协议。标准的控制面API解耦了控制面和数据面的绑定。

最后，在大厂的支持上，Istio 由谷歌和 IBM 共同推出，从应用场景的分析规划到本身的定位，从自身架构的设计到与周边生态的结合，都有着比较严密的论证。Istio项目在发起时已经确认了将云原生生态系统中的容器作为核心，将Kubernetes作为管理容器的编排系统，需要一个系统管理在容器平台上运行的服务之间的交互，包括控制访问、安全、运行数据收集等，而 Istio 正是为此而生的；另外，Istio 成为架构的默认部分，就像容器和Kubernetes已经成为云原生架构的默认部分一样。

云原生社区的定位与多个云厂商的规划也不谋而合。华为云已经在 2018 年 8 月率先在其容器服务CCE（Cloud Container Engine）中内置Istio；Google的GKE也在2018年12月宣布内置 Istio；越来越多的云厂商也已经选择将 Istio作为其容器平台的一部分提供给用户，即提供一套开箱即用的容器应用运行治理的全栈服务。正因为看到了 Istio 在技术和产品上的巨大潜力，各大厂商在社区的投入也在不断加大，其中包括Google、IBM、华为、思科、红帽等主流厂商。

**istio原理图**

![1588498993067](./img/1588498993067.png)

> **总结**
>
> 时代选择服务网格是因为架构的发展
> 服务网格选择istio是因为提供一套开箱即用的容器应用运行治理的全栈服务



## 第三章 Istio 进阶

### 3.1 Istio 架构

Istio的架构，分为控制平面和数据面平两部分。
- 数据平面：由一组智能代理（[Envoy]）组成，被部署为 sidecar。这些代理通过一个通用的策略和遥测中心传递和控制微服务之间的所有网络通信。
- 控制平面：管理并配置代理来进行流量路由。此外，控制平面配置 Mixer 来执行策略和收集遥测数据。

![1589293535050](./img/1589293535050.png)

架构图可以看到，主要分为两个平面，控制面主要包括Istio的一些组件，例如：Pilot、Mixer、Citadel等服务组件；数据面由伴随每个应用程序部署的代理程序Envoy组成，执行针对应用程序的治理逻辑。为了避免静态、刻板地描述组件，在介绍组件的功能前，我们先通过一个动态场景来了解图架构图中对象的工作机制，即观察前端服务对后台服务进行一次访问时，在 Istio 内部都发生了什么，以及 Istio 的各个组件是怎样参与其中的，分别做了哪些事情。

先简单理解

- Pilot：提供服务发现功能和路由规则

- Mixer：策略控制，比如：服务调用速率限制

- Citadel：起到安全作用，比如：服务跟服务通信的加密

- Sidecar/Envoy: 代理，处理服务的流量

架构图上带圆圈的数字代表在数据面上执行的若干重要动作。虽然从时序上来讲，控制面的配置在前，数据面执行在后，但为了便于理解，在下面介绍这些动作时以数据面上的数据流为入口，介绍数据面的功能，然后讲解涉及的控制面如何提供对应的支持，进而理解控制面上组件的对应功能。

（1）**自动注入**：（由架构图得知前端服务跟后端服务都有envoy,我们这里以前端服务envoy为例说明）指在创建应用程序时自动注入 Sidecar代理。那什么情况下会自动注入你？在 Kubernetes场景下创建 Pod时，Kube-apiserver调用管理平面组件的 Sidecar-Injector服务，然后会自动修改应用程序的描述信息并注入Sidecar。在真正创建Pod时，在创建业务容器的同时在Pod中创建Sidecar容器。

```yaml
# 原始的yaml文件
apiVersion: apps/v1
kind: Deployment
spec: 
	containers: 
	- name: nginx  
		image: nginx  
		...省略
```

调用Sidecar-Injector服务之后，yaml文件会发生改变

```yaml
# 原始的yaml文件
apiVersion: apps/v1
kind: Deployment
spec: 
	containers: 
	- name: nginx  
		image: nginx  
		...省略
# 增加一个容器image地址		
	containers: 
	- name: sidecar  
		image: sidecar  
		...省略	
```

总结：会在pod里面自动生产一个代理，业务服务无感知

（2）**流量拦截**：在 Pod 初始化时设置 iptables 规则，当有流量到来时，基于配置的iptables规则拦截业务容器的入口流量和出口流量到Sidecar上。但是我们的应用程序感知不到Sidecar的存在，还以原本的方式进行互相访问。在架构图中，流出前端服务的流量会被 前端服务侧的 Envoy拦截，而当流量到达后台服务时，入口流量被后台服务V1/V2版本的Envoy拦截。

![1589530343836](./img/1589530343836.png)

总结：每个pod中都会有一个代理来来拦截所有的服务流量（不管是入口流量还是出口流量）

（3）**服务发现**：前端服务怎么知道后端服务的服务信息呢？这个时候就需要服务发现了，所以服务发起方的 Envoy 调用控制面组件 Pilot 的服务发现接口获取目标服务的实例列表。在架构图中，前端服务内的 Envoy 通过 控制平面Pilot 的服务发现接口得到后台服务各个实例的地址，为访问做准备。

总结：Pilot提供了服务发现功能，调用方需要到Pilot组件获取提供者服务信息

（4）**负载均衡**：数据面的各个Envoy从Pilot中获取后台服务的负载均衡衡配置，并执行负载均衡动作，服务发起方的Envoy（前端服务envoy）根据配置的负载均衡策略选择服务实例，并连接对应的实例地址。

总结：Pilot也提供了负载均衡功能，调用方根据配置的负载均衡策略选择服务实例

（5）**流量治理**：Envoy 从 Pilot 中获取配置的流量规则，在拦截到 入口 流量和出口 流量时执行治理逻辑。比如说，在架构图中，前端服务的 Envoy 从 Pilot 中获取流量治理规则，并根据该流量治理规则将不同特征的流量分发到后台服务的v1或v2版本。当然，这只是Istio流量治理的一个场景，Istio支持更丰富的流量治理能力。

![1589530357433](./img/1589530357433.png)

总结：Pilot也提供了路由转发规则

（6）**访问安全**：在服务间访问时通过双方的Envoy进行双向认证和通道加密，并基于服务的身份进行授权管理。在架构图中，Pilot下发安全相关配置，在前端模块服务和后端服务的Envoy上自动加载证书和密钥来实现双向认证，其中的证书和密钥由另一个控制平面组件Citadel维护。

总结：Citadel维护了服务代理通信需要的证书和密钥

（7）**服务遥测**：在服务间通信时，通信双方的Envoy都会连接控制平面组件Mixer上报访问数据，并通过Mixer将数据转发给对应的监控后端。比如说，在架构图中，前端模块服务对后端服务的访问监控指标、日志和调用链都可以通过Mixer收集到对应的监控后端。

![1589530368013](./img/1589530368013.png)

总结：Mixer组件可以收集各个服务上的日志，从而可以进行监控

（8）**策略执行**：在进行服务访问时，通过Mixer连接后端服务来控制服务间的访问，判断对访问是放行还是拒绝。在架构图中，数据面在转发服务的请求前调用Mixer接口检查是否允许访问，Mixer 会做对应检查，给代理（Envoy）返回允许访问还是拒绝, 比如：可以对前端模块服务到后台服务的访问进行速率控制。

![1589530377053](./img/1589530377053.png)

总结：Mixer组件可以对服务速率进行控制（也就是限流）

（9）**外部访问**：在架构图中，外部服务通过Gateway访问入口将流量转发到服务前端服务内的Envoy组件，对前端服务的负载均衡和一些流量治理策略都在这个Gateway上执行。

**总结：**这里总结在以上过程中涉及的动作和动作主体，可以将其中的每个过程都抽象成一句话：服务调用双方的Envoy代理拦截流量，并根据控制平面的相关配置执行相应的服务治理动作，这也是Istio的数据平面和控制平面的配合方式。

### 3.2 Istio 组件介绍

#### Pilot

**Pilot在Istio架构中必须要有**

> **思考:** 为什么Envoy能够服务发现？并且Envoy为什么可以流量控制？
>
> 就是因为Pilot存在

**什么是Pilot**

Pilot类似传统C/S架构中的服务端Master，下发指令控制客户端完成业务功能。和传统的微服务架构对比，Pilot 至少涵盖服务注册中心和向数据平面下发规则 等管理组件的功能。

**服务注册中心**

如图下图所示，Pilot 为 Envoy sidecar 提供服务发现、用于智能路由的流量管理功能（例如，A/B 测试、金丝雀发布等）以及弹性功能（超时、重试、熔断器等）。

Pilot本身不做服务注册，它会提供一个API接口，对接已有的服务注册系统，比如Eureka，Etcd等。
说白了，Pilot可以看成它是Sidecar的一个领导

![1589530935237](./img/1589530935237.png)

1. Platform Adapter是Pilot抽象模型的实现版本，用于对接外部的不同平台
2. Polit定了一个抽象模型(Abstract model)，处理Platform Adapter对接外部不同的平台， 从特定平台细节中解耦
3. Envoy API负责和Envoy的通讯，主要是发送服务发现信息和流量控制规则给Envoy 

**流程总结：** service服务C会注册到Pilot注册中心平台适配器(Platform Adapter)模块上（假如对接的是Eureka, 那么service服务C会注册到Eureka里面），然后抽象模型(Abstract model)进行平台细节的解耦并且用于处理Platform Adapter对接外部的不同平台，最后通过Envoy API负责和Envoy的通讯，主要是发送服务发现信息和流量控制规则给Envoy  

**数据平面下发规则**

Pilot 更重要的一个功能是向数据平面下发规则，Pilot 负责将各种规则转换换成 Envoy 可识别的格式，通过标准的 协议发送给 Envoy，指导Envoy完成动作。在通信上，Envoy通过gRPC流式订阅Pilot的配置资源。

Pilot将表达的路由规则分发到 Evnoy上，Envoy根据该路由规则进行流量转发，配置规则和流程图如下所示。

**规则如下：**

```yaml
# http请求
http:
-match: # 匹配
 -header: # 头部
   cookie:
   # 以下cookie中包含group=dev则流量转发到v2版本中
    exact: "group=dev"
  route:  # 路由
  -destination:
    name: v2
  -route:
   -destination:
     name: v1 
```

![1587918004493](./img/1587918004493.png)

#### Mixer

**Mixer在Istio架构中不是必须的**

Mixer分为Policy和Telemetry两个子模块，Policy用于向Envoy提供准入策略控制，黑白名单控制，速率限制等相关策略；Telemetry为Envoy提供了数据上报和日志搜集服务，以用于监控告警和日志查询。

**Telemetry介绍**

Mixer是一个平台无关的组件。Mixer的Telemetry 在整个服务网格中执行访问控制和策略使用，并从 Envoy 代理和其他服务收集遥测数据，流程如下图所示。

![image-20210731164914310](./img/image-20210731164914310.png)

遥测报告上报，比如从Envoy中收集数据[请求数据、使用时间、使用的协议等]，通过Adapater上报给Promethues、Heapster等。说白了，就是数据收集，然后通过adapter上传到监控容器里面。

**policy介绍**

policy是另外一个Mixer服务，和istio-telemetry基本上是完全相同的机制和流程。数据面在转发服务的请求前调用istio-policy的Check接口是否允许访问，Mixer 根据配置将请求转发到对应的 Adapter 做对应检查，给代理返回允许访问还是拒绝。可以对接如配额、授权、黑白名单等不同的控制后端，对服务间的访问进行可扩展的控制。

![1589532747163](./img/1589532747163.png)

> 策略控制：检查请求释放可以运行访问 

#### Citadel

**Citadel在Istio架构中不是必须的**

Istio的认证授权机制主要是由Citadel完成，同时需要和其它组件一起配合，参与到其中的组件还有Pilot、Envoy、Mixer，它们四者在整个流程中的作用分别为：

- Citadel：用于负责密钥和证书的管理，在创建服务时会将密钥及证书下发至对应的Envoy代理中；
- Pilot: 用于接收用户定义的安全策略并将其整理下发至服务旁的Envoy代理中；
- Envoy：用于存储Citadel下发的密钥和证书，保障服务间的数据传输安全；
- Mixer: 负责核心功能为前置条件检查和遥测报告上报;

![1589534351519](./img/1589534351519.png)

具体工作流程可描述如下：

- Kubernetes某集群节点新部署了服务Service，此时集群中有两个Pod被启动，每个Pod由Envoy代理容器和Service容器构成，在启动过程中Istio的Citadel组件会将密钥及证书依次下发至每个Pod中的Envoy代理容器中，以保证后续服务A，B之间的安全通信。
- 用户通过Rules API下发安全策略至Pilot组件，Pilot组件通过Pilot-discovery进程整理安全策略中Kubernetes服务注册和配置信息并以Envoy API方式暴露给Envoy。
- Pod 中的Envoy代理会通过Envoy API方式定时去Pilot拉取安全策略配置信息，并将信息保存至Envoy代理容器中。
- 当pod内的服务相互调用时，会调用各自Envoy容器中的证书及密钥实现服务间的通信，同时Envoy容器还会根据用户下发的安全策略进行更细粒度的访问控制。
- Mixer在整个工作流中核心功能为前置条件检查和遥测报告上报，在每次请求进出服务时，服务中的Envoy代理会向Mixer发送check请求，检查是否满足一些前提条件，比如ACL检查，白名单检查，日志检查等，如果前置条件检查通过，处理完后再通过Envoy向Mixer上报日志，监控等数据，从而完成审计工作。

**使用场景：**

在有一些场景中，对于安全要求是非常高的，比如支付，所以Citadel就是用来保证安全的。

回顾kubernetes API Server的功能：

* 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)；
* 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）;
* 资源配额控制的入口；
* 拥有完备的集群安全机制。

**总结：**

用于负责密钥和证书的管理，在创建服务时会将密钥及证书下发至对应的Envoy代理中。

#### Galley

**Galley在istio架构中不是必须的**

Galley在控制面上向其他组件提供支持。Galley作为负责配置管理的组件，并将这些配置信息提供给管理面的 Pilot和 Mixer服务使用，这样其他管理面组件只用和 Galley打交道，从而与底层平台解耦。

![1589691592459](./img/1589691592459.png)

**galley优点**

- 配置统一管理，配置问题统一由galley负责
- 如果是相关的配置，可以增加复用
- 配置跟配置是相互隔离而且，而且配置也是权限控制，比如组件只能访问自己的私有配置

**MCP协议**

> Galley负责控制平面的配置分发主要依托于一一种协议，这个协议叫（MCP）

MCP提供了一套配置订阅和分发的API，里面会包含这个几个角色：

- source: 配置的提供端，在istio中Galley即是source 说白了就是Galley组件，它提供yaml配置
- sink:配置的消费端，istio组件中Pilot和Mixer都属于sink
- resource: source和sink关注的资源体，也就是yaml配置

![1589691680738](./img/1589691680738.png)

Galley 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。Galley 接管 Istio 获取配置、 处理和分配组件的顶级责任。它将负责将其他的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。

说白了：这样其他控制平面（Pilot和 Mixer）面组件只用和 Galley打交道，从而与底层平台解耦。

#### Sidecar-injector

Sidecar-injector 是负责自动注入的组件，只要开启了自动注入，那么在创建pod的时候就会自动调用Sidecar-injector 服务

配置参数：istio-injection=enabled，我们后面会有案例演示

**在istio中sidecar注入有两种方式**

- 需要使用istioctl命令手动注入   （不需要配置参数：istio-injection=enabled）
- 基于kubernetes自动注入（配置参数：istio-injection=enabled）

手动注入和自动注入会在istio安装之后案例演示

两种区别：

- 手动注入需要每次在执行配置都需要加上istioctl命令

- 自动注入只需要做一下开启参数即可

**sidecar模式具有以下优势**

- 把业务逻辑无关的功能抽取出来（比如通信），可以降低业务代码的复杂度
- sidecar可以独立升级、部署，与业务代码解耦

**注入流程**

在 Kubernetes环境下，根据自动注入配置，Kube-apiserver在拦截到 Pod创建的请求时，会调用自动注入服务 istio-sidecar-injector 生成 Sidecar 容器的描述并将其插入原 Pod的定义中，这样，在创建的 Pod 内, 除了包括业务容器，还包括 Sidecar容器。这个注入过程对用户透明，用户使用原方式创建工作负载。

![1589554243183](./img/1589554243183.png)

**总结：sidecar模式具有以下优势**

- 把业务逻辑无关的功能抽取出来（比如通信），可以降低业务代码的复杂度
- sidecar可以独立升级、部署，与业务代码解耦

#### Proxy(Envoy)

Proxy是Istio数据平面的轻量代理。
Envoy是用C++开发的非常有影响力的轻量级高性能开源服务代理。作为服务网格的数据面，Envoy提供了动态服务发现、负载均衡、TLS、HTTP/2 及 gRPC代理、熔断器、健康检查、流量拆分、灰度发布、故障注入等功能。

Envoy 代理是唯一与数据平面流量交互的 Istio 组件。

**Envoy组件解析**

为了便于理解Istio中Envoy与服务的关系，如图所示：

![1589639648064](./img/1589639648064.png)

一个pod里面运行了一个Envoy容器和service A容器，而Envoy容器内部包含了两个进程，分别是**Pilot-agent和Envoy**两个进程

- **pilot-agent**

  pilot-agent跟Envoy打包在同一个docker镜像里面

  pilot-agent作用

  * 生成envoy配置
  * 启动envoy
  * 监控envoy的运行状态，比如envoy出错是pilot-agent负责重启envoy,huozhe envoy配置变更之后reload envoy

- **Envoy**

  负责拦截pod流量，负责从控制平面pilot组件获取配置和服务发现，上报数据给mixer组件

#### Ingressgateway 

ingressgateway 就是入口处的 Gateway，从网格外访问网格内的服务就是通过这个Gateway进行的。ingressgateway比较特别，是一个Loadbalancer类型的Service，不同于其他服务组件只有一两个端口，ingressgateway 开放了一组端口，这些就是网格内服务的外部访问端口。

网格入口网关ingressgateway和网格内的 Sidecar是同样的执行体，也和网格内的其他 Sidecar一样从 Pilot处接收流量规则并执行。因为入口处的流量都走这个服务。

**流程图如下**

![1589639879534](./img/1589639879534.png)

由于gateway暴露了一个端口，外部的请求就可以根据这个端口把请求发给gateway了然后由gateway把请求分发给网格内部的pod上

#### 其他组件

在Istio集群中一般还安装grafana、Prometheus、Tracing组件，这些组件提供了Istio的调用链、监控等功能，可以选择安装来完成完整的服务监控管理功能。

**总结**

主要介绍了一些常见的istio组件，其中有一些组件是istio默认就已经使用了，有一些组件我们后面也会来演示。

### 3.3 Istio 安装

Istio支持在不同的平台下安装其控制平面，例如Kubernetes、Mesos和虚拟机等。

课程上以 Kubernetes 为基础讲解如何在集群中安装 Istio （Istio 1.0.6 要求Kubernetes的版本在1.11及以上）。

可以在本地或公有云上搭建Istio环境，也可以直接使用公有云平台上已经集成了Istio的托管服务。

#### Kubernetes集群环境

具体安装步骤参考：https://pyygithub.github.io/study/k8s/kubernetes01.html#_2-1-%E7%8E%AF%E5%A2%83%E8%A7%84%E5%88%92

#### 安装 Istio

在Istio的版本发布页面https://github.com/istio/istio/releases/tag/1.0.6下载安装包并解压（我用的是一个比较稳定的版本1.0.6版本，放到master上面，以Linux平台的istio-1.0.6-linux.tar.gz为例）

1.解压tar -xzf istio-1.0.6-linux.tar.gz

2.进入istio目录cd  istio-1.0.6/

**Istio的安装目录及其说明**

| 文件/文件夹   | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| bin           | 包含客户端工具,用于和Istio APIS交互                          |
| install       | 包含了Consul和Kubernetes平台的Istio安装脚本和文件，在Kubernetes平台上分为YAML资源文件和Helm安装文件 |
| istio.VERSION | 配置文件包含版本信息的环境变量                               |
| samples       | 包含了官方文档中用到的各种应用实例如bookinfo/helloworld等等，这些示例可以帮助读者理解Istio的功能以及如何与Istio的各个组件进行交互 |
| tools         | 包含用于性能测试和在本地机器上进行测试的脚本文件和工具       |

**有以下几种方式安装Istio：**

* 使用install/kubernetes文件夹中的istio-demo.yaml进行安装；
* 使用Helm template渲染出Istio的YAML安装文件进行安装；
* 使用Helm和Tiller方式进行安装。
