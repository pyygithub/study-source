(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{201:function(t,a,s){t.exports=s.p+"assets/img/namenode.f626aa82.png"},202:function(t,a,s){t.exports=s.p+"assets/img/HDFS.fa7eae62.png"},207:function(t,a,s){"use strict";s.r(a);var n=s(28),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,n=t._self._c||a;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"hadoop-笔记"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-笔记"}},[t._v("#")]),t._v(" Hadoop 笔记")]),t._v(" "),n("h2",{attrs:{id:"大数据概念"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#大数据概念"}},[t._v("#")]),t._v(" 大数据概念")]),t._v(" "),n("h3",{attrs:{id:"什么是大数据？"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#什么是大数据？"}},[t._v("#")]),t._v(" 什么是大数据？")]),t._v(" "),n("blockquote",[n("p",[t._v("KB - MB - GB - TB - PB - EB - ZB - YB - DB - NB")])]),t._v(" "),n("p",[t._v("一般来说达到 TB ，或日增达到 GB 就属于大数据了\nMySQL 单表超过 500万 条数据")]),t._v(" "),n("h3",{attrs:{id:"大数据特点？"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#大数据特点？"}},[t._v("#")]),t._v(" 大数据特点？")]),t._v(" "),n("p",[n("strong",[t._v("大数据的 5V 特性：")])]),t._v(" "),n("ul",[n("li",[n("p",[n("strong",[t._v("Volume：巨大的数据量")]),t._v("\n集中储存/集中计算已经无法处理巨大的数据量。\n数据量呈指数增长：地震、录井 、石油钻塔的传感器一个月产生的数据量比全球所有的电影加在一起还要多。\n新浪微博用户数2.5亿+，高峰每天几亿条。")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("Variety：非结构化数据多样性")]),t._v("\n文本/图片/视频/文档等，如诸如微地震，电磁以及光纤分布式温度监测（DTS） 。")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("Velocity：数据增长速度快")]),t._v("\n用户基数庞大/设备数量众多/实时海量/数据指数级别增长。")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("Valueless：数据价值密度低")]),t._v("\n每个钻井平台有 40,000 传感器,但是通常只有 10% 的数据使用到。\n每个深水钻井平台的投资可达到$150M,能有效利用所有的数据非常关键,关系到安全与优化运营 。")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("Veracity：数据质量")]),t._v("\n数据的准确性和可信赖度，即数据的质量。")])])]),t._v(" "),n("h3",{attrs:{id:"面临了哪些问题-如何解决？"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#面临了哪些问题-如何解决？"}},[t._v("#")]),t._v(" 面临了哪些问题,如何解决？")]),t._v(" "),n("ul",[n("li",[t._v("数据如何存储 - HDFS")]),t._v(" "),n("li",[t._v("数据如何计算 - MapReduce")]),t._v(" "),n("li",[t._v("资源如何管理（CPU 内存 网络资源）- YARN")])]),t._v(" "),n("h3",{attrs:{id:"大数据应用场景"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#大数据应用场景"}},[t._v("#")]),t._v(" 大数据应用场景")]),t._v(" "),n("ul",[n("li",[n("p",[n("strong",[t._v("个人推荐")]),t._v("\n根据用户喜好，推荐相关兴趣内容\n千人一面：范围广、精度粗\n一人一面：范围小、精度高\n一人千面：兴趣内容范围大、精度高")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("风控")]),t._v("\n金融系统、银行、互联网金融 - 实时流处理")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("成本预测")]),t._v("\n根据近期销售和市场数据，预测成本，做出规划")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("气候预测")]),t._v("\n根据以往气象信息，预测近期气象变化，和推测之后气候异常")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("人工智能")]),t._v("\n无人汽车：百度、特斯拉、Google\n智能助手：小爱、小度\n物流机器人")])])]),t._v(" "),n("h2",{attrs:{id:"hadoop-的诞生"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-的诞生"}},[t._v("#")]),t._v(" Hadoop 的诞生")]),t._v(" "),n("p",[t._v("由 Apache 组织提供的一个开源的大数据解决方案。")]),t._v(" "),n("h3",{attrs:{id:"hadoop-的起源"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-的起源"}},[t._v("#")]),t._v(" Hadoop 的起源")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("2003-2004年，Google公布了部分GFS和MapReduce思想的细节，受此启发的Doug Cutting等人用2年的业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。然后Yahoo招安Doug Gutting及其项目。")])]),t._v(" "),n("li",[n("p",[t._v("2005年，Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。")])]),t._v(" "),n("li",[n("p",[t._v("2006年2月被分离出来，成为一套完整独立的软件，起名为Hadoop。Hadoop名字不是一个缩写，而是一个生造出来的词。是Hadoop之父Doug Cutting儿子毛绒玩具象命名的。")])]),t._v(" "),n("li",[n("p",[t._v("Hadoop的成长过程\nLucene–>Nutch—>Hadoop")])]),t._v(" "),n("li",[n("p",[t._v("总结起来，Hadoop起源于Google的三大论文")]),t._v(" "),n("ul",[n("li",[t._v("GFS：Google的分布式文件系统Google File System")]),t._v(" "),n("li",[t._v("MapReduce：Google的MapReduce开源分布式并行计算框架")]),t._v(" "),n("li",[t._v("BigTable：一个大型的分布式数据库")])])]),t._v(" "),n("li",[n("p",[t._v("演变关系\nGFS—->HDFS\nGoogle MapReduce—->Hadoop MapReduce\nBigTable—->HBase")])])]),t._v(" "),n("p",[t._v("狭义上来说，hadoop就是单独指代hadoop这个软件，\n广义上来说，hadoop指代大数据的一个生态圈，包括很多其他的软件")]),t._v(" "),n("h3",{attrs:{id:"hadoop-的历史版本"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-的历史版本"}},[t._v("#")]),t._v(" Hadoop 的历史版本")]),t._v(" "),n("ul",[n("li",[n("strong",[t._v("0.x系列版本")]),t._v("：hadoop当中最早的一个开源版本，在此基础上演变而来的1.x以及2.x的版本")]),t._v(" "),n("li",[n("strong",[t._v("1.x版本系列")]),t._v("：hadoop版本当中的第二代开源版本，主要修复0.x版本的一些bug等")]),t._v(" "),n("li",[n("strong",[t._v("2.x版本系列")]),t._v("：架构产生重大变化，引入了yarn平台等许多新特性")])]),t._v(" "),n("h3",{attrs:{id:"hadoop-生态圈"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-生态圈"}},[t._v("#")]),t._v(" Hadoop 生态圈")]),t._v(" "),n("ul",[n("li",[n("strong",[t._v("HDFS")]),t._v("：Hadoop Distribute FileSystem")]),t._v(" "),n("li",[n("strong",[t._v("MapReduce")]),t._v("：Hadoop中的分布式计算框架，实现对海量数据的并行分析和计算。")]),t._v(" "),n("li",[n("strong",[t._v("Hbase")]),t._v("：基于HDFS的列式存储的 NoSQL 数据库。")]),t._v(" "),n("li",[n("strong",[t._v("Hive")]),t._v("：简化大数据开发，可以将 SQL 语法翻译成 MR 任务。")]),t._v(" "),n("li",[n("strong",[t._v("Flume")]),t._v("：分布式的日志收集系统，用于收集海量数据，将其存储到 FS 中。")]),t._v(" "),n("li",[n("strong",[t._v("Kafka")]),t._v("：分布式的消息系统，实现分布式解耦和海量数据缓冲。")]),t._v(" "),n("li",[n("strong",[t._v("Zookeeper")]),t._v("：分布式协调服务，用于服务注册中心、配置中心、集群选举、状态监测、分布式锁等。")])]),t._v(" "),n("h2",{attrs:{id:"大数据解决方案"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#大数据解决方案"}},[t._v("#")]),t._v(" 大数据解决方案")]),t._v(" "),n("ul",[n("li",[n("strong",[t._v("MR")]),t._v("：代表基于"),n("strong",[t._v("磁盘")]),t._v("的大数据离线批处理的解决方案 - 延迟较高")]),t._v(" "),n("li",[n("strong",[t._v("Spark")]),t._v("：代表基于"),n("strong",[t._v("内存")]),t._v("的大数据静态批处理的解决方案 - 几乎是MR的10倍以上")]),t._v(" "),n("li",[n("strong",[t._v("Storm/Spark Streaming/Flink/Kafka Streaming")]),t._v("：实时流处理框架，达到对记录级别的数据显示和毫秒级处理")])]),t._v(" "),n("h2",{attrs:{id:"hdfs-分布式系统配置"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-分布式系统配置"}},[t._v("#")]),t._v(" HDFS 分布式系统配置")]),t._v(" "),n("p",[n("strong",[t._v("核心配置参数：")])]),t._v(" "),n("ul",[n("li",[t._v("指定 hadoop 的默认文件系统为：hdfs")]),t._v(" "),n("li",[t._v("指定 hdfs 的 namenode 节点是哪台机器")]),t._v(" "),n("li",[t._v("指定 namenode 软件存储元数据的本地目录")]),t._v(" "),n("li",[t._v("指定 datanode 软件存储文件块的本地目录")])]),t._v(" "),n("ol",[n("li",[n("p",[t._v("环境配置文件hadoop-env.sh")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# The java implementation to use.")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("export")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("JAVA_HOME")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("/opt/soft/jdk1.8.0_211\n"),n("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("export")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("HADOOP_CONF_DIR")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token variable"}},[t._v("${HADOOP_CONF_DIR"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":-")]),t._v("$PWD}")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Hadoop配置文件的存放目录")]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("核心配置文件 core-site.xml")]),t._v(" "),n("div",{staticClass:"language-xml extra-class"},[n("pre",{pre:!0,attrs:{class:"language-xml"}},[n("code",[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("configuration")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- fs.defaultFS: 默认文件系统 hdfs  --\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("fs.defaultFS"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hdfs://hdp-01:9000"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" \n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("configuration")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("HDFS配置文件hdfs-site.xml")]),t._v(" "),n("div",{staticClass:"language-xml extra-class"},[n("pre",{pre:!0,attrs:{class:"language-xml"}},[n("code",[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("configuration")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- namenode 地址 --\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.namenode.http-address"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hdp-01:50070"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- secondary namenode 地址 --\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.namenode.http-address"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hdp-01:50090"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    \n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 指定 namenode 软件存储元数据的本地目录 格式化节点时会自动生成--\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.namenode.name.dir"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("/root/hdpdata/name"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- 指定 datanode 软件存储文件块的本地目录 格式化节点时会自动生成   --\x3e")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.datanode.data.dir"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("/root/hdpdata/data"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" \n\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("configuration")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("配置datanode集群节点文件 slaves")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("hdp-01\nhdp-02\nhdp-03\n")])])])]),t._v(" "),n("li",[n("p",[t._v("配置好以上信息后，我们就可以将hadoop的包分发给其他的节点了")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[n("span",{pre:!0,attrs:{class:"token function"}},[t._v("scp")]),t._v(" -r hadoop-2.x.x root@hdp02:"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("目标路径"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("启动集群")]),t._v(" "),n("p",[t._v("在主节点上运行")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("hadoop namenode -format\n")])])]),n("p",[t._v("运行完成后，节点会自动生成刚刚配置的工作目录")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("start-dfs.sh\n")])])])]),t._v(" "),n("li",[n("p",[t._v("浏览器输入http://x.x.x.x:50070查看集群运行情况")]),t._v(" "),n("p",[n("img",{attrs:{src:s(201),alt:"namenode"}})])]),t._v(" "),n("li",[n("p",[t._v("最终服务器分布：")]),t._v(" "),n("table",[n("thead",[n("tr",[n("th",[t._v("服务器地址")]),t._v(" "),n("th",[t._v("端口")]),t._v(" "),n("th",[t._v("服务")])])]),t._v(" "),n("tbody",[n("tr",[n("td",[t._v("hdp-01")]),t._v(" "),n("td",[t._v("50070"),n("br"),t._v("50090")]),t._v(" "),n("td",[t._v("namenode"),n("br"),t._v("secondaryNamenode"),n("br"),t._v("datanode")])]),t._v(" "),n("tr",[n("td",[t._v("hdp-02")]),t._v(" "),n("td",[t._v("50010/50075")]),t._v(" "),n("td",[t._v("datanode")])]),t._v(" "),n("tr",[n("td",[t._v("hdp-03")]),t._v(" "),n("td",[t._v("50010/50075")]),t._v(" "),n("td",[t._v("datanode")])])])])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("问题总结")])]),t._v(" "),n("ul",[n("li",[t._v("防火墙设置\n为了防止发生一些奇怪的错误，请务必关闭所有节点的防火墙，他可能会导致浏览器无法获取集群信息和文件上传集群失败")]),t._v(" "),n("li",[t._v("hosts文件配置和主机名\n因为这是完全分布式的集群，所以配置hosts文件至关重要，不然你的私钥配置和以后节点的格式化都会出错，他将会提示你无法解析主机名")]),t._v(" "),n("li",[t._v("请在关闭所有HDFS服务后在执行"),n("code",[t._v("-format")]),t._v("格式化命令\n如果存在节点未关闭，而你运行了格式化命令，这可能导致该节点与其他节点的目录ID不一致，从而导致“网络分区”问题")])])])]),t._v(" "),n("h2",{attrs:{id:"hdfs-体系架构"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-体系架构"}},[t._v("#")]),t._v(" HDFS 体系架构")]),t._v(" "),n("p",[n("a",{attrs:{href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("官方文档"),n("OutboundLink")],1)]),t._v(" "),n("p",[t._v("HDFS 是一种能够运行在商业硬件上的分布式文件系统，与目前市面上文件系统有很多相似之处，但是又是不同的软件系统。在HDFS中，使用的架构主从架构（Active|Standby），针对的是"),n("strong",[t._v("NameNode")]),t._v("的高可用。")]),t._v(" "),n("ol",[n("li",[n("p",[n("strong",[t._v("NameNode")])]),t._v(" "),n("ul",[n("li",[t._v("存储文件的元数据如"),n("code",[t._v("文件名、文件目录结构、文件属性(生成时间、福本数、文件权限) ，以及每个文件的块列表和块所在的DataNode映射")])]),t._v(" "),n("li",[t._v("负责管理 DataNode")]),t._v(" "),n("li",[t._v("控制外界客户端对文件系统的访问。")]),t._v(" "),n("li",[t._v("NameNode会启动一个（或者说一个集群中，只有一个NameNode节点运行）")])])]),t._v(" "),n("li",[n("p",[t._v("**DataNode **\n在本地文件系统存储文件块数据，以及块数据的校验和，负责响应客户端对块的读写请求。")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("Block")]),t._v("\n数据块，是对文件拆分的最小单元（默认情况下一个块大小128MB，每个数据块有3个副本）")])]),t._v(" "),n("li",[n("p",[n("strong",[t._v("rack")])]),t._v(" "),n("p",[t._v("机架，使用机架对存储节点进行物理编排，用于优化存储和计算")]),t._v(" "),n("p",[n("img",{attrs:{src:s(202),alt:"img"}})])])]),t._v(" "),n("h3",{attrs:{id:"什么是block块"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#什么是block块"}},[t._v("#")]),t._v(" 什么是Block块")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("概述")]),t._v(" "),n("p",[t._v("hadoop集群中文件的存储都是以块的形式存储在hdfs中，且一般默认都会有三个备份分别存储在集中中不同的datanode节点上。")])]),t._v(" "),n("li",[n("p",[t._v("默认值")]),t._v(" "),n("p",[t._v("从2.7.3版本开始 block size 默认值大小为 128M，之前版本默认值为64M")])]),t._v(" "),n("li",[n("p",[t._v("如何修改 block 块的大小？")]),t._v(" "),n("p",[t._v("可以通过修改 hdfs-site.xml中的 dfs.blocksize 对应的值")]),t._v(" "),n("p",[t._v("注意: 在修改HDFS的数据块大小时，首先停掉集群hadoop的运行，修改完毕后重新启动。")])]),t._v(" "),n("li",[n("p",[t._v("block块的大小设置规则")]),t._v(" "),n("p",[t._v("在实际应用中，hdfs block块的大小设置为多少合适呢？为什么有的是64M，有个的是128M、256M、512M呢？")]),t._v(" "),n("p",[t._v("首先我们先来了解几个概念：")]),t._v(" "),n("ol",[n("li",[t._v("寻址时间：HDFS 中找到目标文件block所花费的时间。")]),t._v(" "),n("li",[t._v("原理：文件块越大，寻址时间越短，但磁盘传输时间越长；文件块越小，寻址时间越长，但磁盘传输时间越短。")])])]),t._v(" "),n("li",[n("p",[t._v("block不能设置过大，也不能设置过小")]),t._v(" "),n("ol",[n("li",[t._v("如果设置过大，一方面从磁盘传输数据的时间会明显大于寻址时间，导致程序处理这块数据时，变得非常慢；另一方面，MapReduce 中map任务通常一次只处理一个块中的数据，如果过大运行速度也会很慢。")]),t._v(" "),n("li",[t._v("如果设置过小，一方面存放大小小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；另一方面块过小，寻址时间增长，导致程序一直在找block的开始位置。因此，块适当设置大一些，减少寻址时间，那么传输一个有多个块组成的文件时间"),n("strong",[t._v("主要取决于磁盘的传输速度")]),t._v("。")])])]),t._v(" "),n("li",[n("p",[t._v("设置多大合适？")]),t._v(" "),n("ol",[n("li",[n("p",[t._v("HDFS中平均寻址时间大概为 10ms；")])]),t._v(" "),n("li",[n("p",[t._v("经过大量的测试发现，寻址时间为传输时间的1%时，为最佳状态，所以最佳的传输时间为：")]),t._v(" "),n("p",[t._v("10ms/0.01 = 1000s = 1s")])]),t._v(" "),n("li",[n("p",[t._v("目前磁盘的传输普遍为100MB/s，最佳block大小计算：")]),t._v(" "),n("p",[t._v("100MB/s*1s=100MB")]),t._v(" "),n("p",[t._v("所以我们设置block大小为 128MB。")])]),t._v(" "),n("li",[n("p",[t._v("实际中，磁盘的传输速率为 200MB/s时，一般设定block块大小为256MB；磁盘传输速率为400MB/s时，一般设定block大小为512MB.")])]),t._v(" "),n("li",[n("p",[t._v("g")])])])])]),t._v(" "),n("h3",{attrs:{id:"什么是机架感知？"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#什么是机架感知？"}},[t._v("#")]),t._v(" 什么是机架感知？")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("背景")]),t._v(" "),n("p",[t._v("分布式的集群通常包含非常多的机器，由于受到机架槽位和交换机网口的限制，通常大型的分布式集群都会跨好多个机架，由多个机架上的机器共同组成分布式集群。机架内的机器之间的网络速度通常都会高于跨机架机器之间的网络速度，并且机架之间机器的网络通信通常受到上层交换机网络带宽的限制。")])]),t._v(" "),n("li",[n("p",[t._v("存储策略")]),t._v(" "),n("p",[n("strong",[t._v("Hadoop在设计时考虑到数据的安全和高效，数据文件默认在HDFS上存放三份，存储策略为：")])]),t._v(" "),n("p",[t._v("第一个block副本放在客户端所在的数据节点上（如果客户端不在集群范围内，则从整个集群中随机选择一个合适的数据节点来存放）。")]),t._v(" "),n("p",[t._v("第二个副本放置在与第一个副本所在节点相同的机架内的其它数据节点上")]),t._v(" "),n("p",[t._v("第三个副本放置在不同机架节点上")]),t._v(" "),n("p",[t._v("如果副本数  >= 4时：")]),t._v(" "),n("p",[t._v("前三个副本按照上面原则存放，从第四个副本开始，随机选取dataNode节点存放（每个节点只保留一份副本，每个rack不超过两个副本）")]),t._v(" "),n("p",[t._v("这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上获取数据的速度快；")]),t._v(" "),n("p",[t._v("同时，如果整个机架的网络出现异常，也能保障可以在其它机架节点上找到数据。")]),t._v(" "),n("p",[t._v("为了降低整体的带宽消耗和读取延时，HDFS会尽量让读取程序读取离它最近的副本。")]),t._v(" "),n("p",[t._v("如果在读取程序的同一个机架上有一个副本，那么就读取该副本。")]),t._v(" "),n("p",[t._v("如果在一个HDFS集群跨越多个数据中心，那么客户端也将读取本地数据中心的副本。")]),t._v(" "),n("p",[t._v("那么Hadoop是如何确定任意两个节点为于同一个机架，还是跨机架的呢？答案就是"),n("strong",[t._v("机架感知")]),t._v("。")]),t._v(" "),n("p",[t._v('默认情况下，hadoop的机架感知是没有被开启的。所有机器hadoop都默认同一个机架下，名为“、default-rack",这种情况下，任何一台 datanode 机器，不管物理上是否属于同一个机架，都会被认为在同一个机架下，此时，就很容易出现增添机架间网络负载的情况。因为此时hadoop集群的HDFS在选机器的时候，是随机选择的，也就是说，很可能在写数据的时候，hadoop将第一块数据的block1写在 rack1上，然后随机的选择将block2写入到rack2上。')]),t._v(" "),n("p",[t._v("此时，两个rack之间产生了数据传输的流量，再接下来，在随机的情况下，又将block3重新又写回到rack2，此时，两个rack之间又产生了一次数据流量。")]),t._v(" "),n("p",[t._v("在job处理的数据量非常大的时候，或者hadoop推送的数据量非常大的时候，这种情况会造成rack之间的网络流量成倍的上升，成为性能的瓶颈，进而影响作业的性能甚至于整个集群的服务。")])]),t._v(" "),n("li",[n("p",[t._v("配置")]),t._v(" "),n("p",[t._v("具体配置可参考：https://blog.csdn.net/zhongqi2513/article/details/73695229")])])]),t._v(" "),n("h2",{attrs:{id:"hdfs-命令行客户端基本操作"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-命令行客户端基本操作"}},[t._v("#")]),t._v(" HDFS 命令行客户端基本操作")]),t._v(" "),n("h3",{attrs:{id:"客户端理解"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#客户端理解"}},[t._v("#")]),t._v(" 客户端理解")]),t._v(" "),n("p",[t._v("HDFS 的客户端有多种形式：")]),t._v(" "),n("ul",[n("li",[t._v("网页形式")]),t._v(" "),n("li",[t._v("命令行形式")])]),t._v(" "),n("p",[t._v("客户端在哪里运行，没有约束，只要运行客户端的机器能够跟 HDFS 集群联网。")]),t._v(" "),n("blockquote",[n("p",[t._v("注意：文件的切块大小和存储的福本数量，都是由客户端决定的！")])]),t._v(" "),n("p",[t._v("所谓客户端决定，是通过配置参数类定的。HDFS 的客户端会读取以下两个参数，来决定切块大小和副本数量：")]),t._v(" "),n("ul",[n("li",[t._v("切块大小参数：dfs.blocksize")]),t._v(" "),n("li",[t._v("副本数量的参数：dfs.replication")])]),t._v(" "),n("p",[t._v("上面两个参数在 hdfs-site.xml 中配置：")]),t._v(" "),n("div",{staticClass:"language-xml extra-class"},[n("pre",{pre:!0,attrs:{class:"language-xml"}},[n("code",[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.blocksize"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("128m"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.replication"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("3"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),n("h3",{attrs:{id:"hdfs-命令行客户端的常用操作命令"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-命令行客户端的常用操作命令"}},[t._v("#")]),t._v(" HDFS 命令行客户端的常用操作命令")]),t._v(" "),n("ol",[n("li",[n("p",[t._v("查看 HDFS 目录信息")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("hadoop fs -ls /hdfs目录\n")])])])]),t._v(" "),n("li",[n("p",[t._v("在 HDFS 中创建文件夹")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -mkdir -p /aa/bb/cc\n")])])])]),t._v(" "),n("li",[n("p",[t._v("移动 HDFS 中的文件（改名）")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -mv /hdfs路径\t/hdfs另一个路径\n")])])])]),t._v(" "),n("li",[n("p",[t._v("上传文件到 HDFS 中")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -put /本地文件 /hdfs目录\nhadoop fs -copyFromLocal/本地文件\t/hdfs目录   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## copyFromLocak == put")]),t._v("\nhadoop fs -moveFromLocal/本地文件\t/hdfs目录\t\t"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## 从本地移动到hdfs")]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("下载文件到客户端本地")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -get /hdfs中的文件\t/本地目录\nhadoop fs -copyToLocal/hdfs中的文件\t/本地目录\t"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## copyToLocal == get")]),t._v("\nhadoop fs -moveToLocal/hdfs中的文件 /本地目录\t"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("## 从 hdfs 中移动到本地")]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("删除 HDFS 中的文件或文件夹")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("rm")]),t._v(" -r /hdfs目录或文件\n")])])])]),t._v(" "),n("li",[n("p",[t._v("修改文件的权限")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -chown user:group /hdfs文件\nhadoop fs -chmod "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("777")]),t._v(" /hdfs文件\n")])])])]),t._v(" "),n("li",[n("p",[t._v("追加内容到已有文件")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -appendToFile /本地文件\t/hdfs文件\n")])])])]),t._v(" "),n("li",[n("p",[t._v("显示文本文件内容")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[t._v("hadoop fs -cat /hdfs文件\nhadoop fs -tail /hdfs文件 \n")])])])])]),t._v(" "),n("h2",{attrs:{id:"hdfs-的-javaapi"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-的-javaapi"}},[t._v("#")]),t._v(" HDFS 的 JavaAPI")]),t._v(" "),n("h3",{attrs:{id:"开发环境准备"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#开发环境准备"}},[t._v("#")]),t._v(" 开发环境准备")]),t._v(" "),n("ol",[n("li",[t._v("在本地解决Hadoop安装包")]),t._v(" "),n("li",[t._v("在环境变量中配置：HADOOP_HOME")]),t._v(" "),n("li",[t._v("创建 SpringBoot 工程,引入 Hadoop 依赖包")])]),t._v(" "),n("div",{staticClass:"language-xml extra-class"},[n("pre",{pre:!0,attrs:{class:"language-xml"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- hadoop-hdfs --\x3e")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("dependency")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("groupId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("org.apache.hadoop"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("groupId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("artifactId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hadoop-hdfs"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("artifactId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("version")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("2.10.0"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("version")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("dependency")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!-- hadoop-common --\x3e")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("dependency")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("groupId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("org.apache.hadoop"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("groupId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("artifactId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("hadoop-common"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("artifactId")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("version")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("2.10.0"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("version")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("dependency")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),n("h3",{attrs:{id:"文件上传"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#文件上传"}},[t._v("#")]),t._v(" 文件上传")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conf")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fs")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FSDataOutputStream")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fs")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSystem")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fs")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOUtils")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("junit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("After")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("junit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Before")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("junit")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Test")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("java"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("File")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("java"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputStream")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("java"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileNotFoundException")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("java"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("App")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),t._v(" configuration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSystem")]),t._v(" fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Before")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getClient")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("setProperty")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HADOOP_USER_NAME"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"root"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        configuration "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Configuration")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 手动设置连接信息")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('//configuration.set("fs.defaultFS", "")')]),t._v("\n        configuration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("addResource")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"core-site.xml"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        configuration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("addResource")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs-site.xml "')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n        fs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSystem")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("newInstance")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("configuration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@After")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("close")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("close")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("upload1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("copyFromLocalFile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/本地文件"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("upload2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Exception")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputStream")]),t._v(" inputStream "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputStream")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("File")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/本地文件"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FSDataOutputStream")]),t._v(" outputStream "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("create")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/a.txt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOUtils")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("copyBytes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputStream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" outputStream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[n("strong",[t._v("权限不足解决方案")])]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("org"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("security"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("AccessControlException")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Permission")]),t._v(" denied"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" user"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("wolf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" access"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("WRITE"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inode"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/"')]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("root"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("supergroup"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("drwxr"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("xr"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("x\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])])]),n("ul",[n("li",[n("p",[t._v("方案一：")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("setProperty")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"HADOOP_USER_NAME"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"root"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("方案二：")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("java -jar xxxx -DHADOOP_USER_NAME"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("root\n")])])])]),t._v(" "),n("li",[n("p",[t._v("方案三：")]),t._v(" "),n("blockquote",[n("p",[t._v("将权限检查机制关闭：ect/hadoop/hdfs-site.xml:")])]),t._v(" "),n("div",{staticClass:"language-xml extra-class"},[n("pre",{pre:!0,attrs:{class:"language-xml"}},[n("code",[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("name")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("dfs.permissions.enabled"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("name")]),t._v("\n\t"),n("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("<value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("false"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("value")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token tag"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("property")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])])])]),t._v(" "),n("h3",{attrs:{id:"下载文件"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#下载文件"}},[t._v("#")]),t._v(" 下载文件")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("download1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("copyToLocalFile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/hdfs文件"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/本地目录"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("download2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputStream")]),t._v(" inputStream "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileInputStream")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("File")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hdfs文件"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FSDataOutputStream")]),t._v(" outputStream "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("create")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/本地目录"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOUtils")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("copyBytes")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputStream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" outputStream"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("h3",{attrs:{id:"创建文件夹"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#创建文件夹"}},[t._v("#")]),t._v(" 创建文件夹")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("\t\t"),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("create")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("mkdirs")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/a/b"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("h3",{attrs:{id:"删除文件"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#删除文件"}},[t._v("#")]),t._v(" 删除文件")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("\t\t"),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("delete")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 递归删除")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("boolean")]),t._v(" delete "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("delete")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/a"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("h3",{attrs:{id:"递归查询目录下所有文件列表"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#递归查询目录下所有文件列表"}},[t._v("#")]),t._v(" 递归查询目录下所有文件列表")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("    "),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("listFile")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("RemoteIterator")]),n("span",{pre:!0,attrs:{class:"token generics"}},[n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LocatedFileStatus")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" remoteIterator "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("listFiles")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("remoteIterator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("hasNext")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("LocatedFileStatus")]),t._v(" fileStatus "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" remoteIterator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("next")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" name "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fileStatus"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getPath")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("getName")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("println")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("h3",{attrs:{id:"判断文件是否存在"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#判断文件是否存在"}},[t._v("#")]),t._v(" 判断文件是否存在")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("\t\t"),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("exist")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("boolean")]),t._v(" exists "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("exists")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/a.txt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("println")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("exists"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("h3",{attrs:{id:"回收站"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#回收站"}},[t._v("#")]),t._v(" 回收站")]),t._v(" "),n("div",{staticClass:"language-java extra-class"},[n("pre",{pre:!0,attrs:{class:"language-java"}},[n("code",[t._v("\t\t"),n("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Test")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("trash")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Trash")]),t._v(" trash "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Trash")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" configuration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("boolean")]),t._v(" b "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" trash"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("moveToTrash")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/a.txt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("System")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("println")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("·")])])}),[],!1,null,null,null);a.default=e.exports}}]);